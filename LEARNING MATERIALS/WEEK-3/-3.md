# Expectations

This week, you are expected to share some of your development with your mentor and peers. By now, you should have started your initial consultations with your mentor. You are expected to present some of your proposed architecture, modules, network design or data processing stages, whichever is applicable, this week. This will help you stay on track with your project management while ensuring quality output.

The first two weeks were about figuring out what a good platform technology is like and how poor platform technology can break operations and result in financial losses for companies. The next ones are about building your contribution to Project Finer FinMark.

As you use this guide, do the following:

1. Take note of any concept, topic, or terminology that is confusing, unclear, and/or unfamiliar to you. Conduct internet research, post a message and interact with your peers on your decided class platform, or schedule a mentoring session.
2. Manage your time wisely so you can complete the readings and/or activities in Camu before your next synchronous session.

---

# Expected Output

You are expected to complete the following tasks by the end of the week. Take note of the ones that are required to be submitted on Camu.

1. Present initial drafts, diagrams, and processes during consultation sessions.
2. Answer items listed in your Initial Consulting Report.

Continue working on your Project Management assignments and timelines.

---

# What are the Basic Software Architecture Concepts?

It pays to know some basic software architecture concepts, especially that you have some answers to your key considerations in how you’ll help in building a finer Finmark. Read this section intently if you are working as a Software Developer of the Project. Otherwise, know the basics of this chapter in order to link them to your specialization.

1. Software Architecture

Software architecture refers to the high-level design of a software system. It defines the system's overall structure, how its parts interact, and how it fits within its environment. It includes the organization of modules or components, the behavior of these components, and how they are assembled into more complex subsystems. In [Gupta](https://www.turing.com/blog/software-architecture-patterns-types#_what_is_software_architecture?) (2024), it’s worth noting how software architecture is given importance to this extent:

The foundation for how you will handle performance, fault tolerance, scalability, and dependability in the future is laid by having great architecture. As you scale up, choosing the appropriate architecture for your software will result in more reliable performance under challenging circumstances.

In essence, software architecture provides the strategic blueprint of a system—guiding both its development and long-term evolution. Consider the typical work of how architects design a reliable structure that can withstand local conditions such as typhoons, power interruptions, and connectivity issues. Architects choose the best materials and ensure that all materials actually match to fulfill the same goal of building a good structure. A strong software architecture similarly prepares a system to function efficiently even under challenging scenarios.

The importance of software architecture lies in how it establishes the foundation for key system qualities such as the following:

1. Performance – ensuring the system responds efficiently to users.
2. Scalability – the ability to handle increasing numbers of users or transactions.
3. Fault Tolerance – the capacity to continue operating despite minor failures.
4. Dependability – consistent and reliable system behavior over time.

You may not have realized it, but you encounter software architecture every day. Watch this [video](https://drive.google.com/file/d/1coxFdkOohzrCFM9t1jbLG_C1ZnbVvVSK/view?usp=drive_link) to know your daily encounters with Software Architecture.

Choosing an appropriate software architecture is crucial because it directly affects how well a system can function under real-world conditions, especially in critical and high-impact sectors. In the Philippines, for example, certain types of systems—such as government e-services, mobile banking applications, and educational platforms—require thoughtful architectural planning due to unique local challenges. These systems promise services, many of them are even paid, in order to function well. However, you have to understand that choosing an architecture becomes an even more difficult task due to conditions such as the following:

1. Unstable Internet Connectivity – Many areas, especially in provinces and remote islands, experience inconsistent or slow internet speeds. A good architecture can allow for offline capabilities or minimal data usage, ensuring the system remains usable despite poor connectivity.
2. Limited Infrastructure – Some public institutions and rural schools may lack modern hardware or high-performance devices. A system’s architecture that targets even those areas with this challenge should be lightweight and optimized for low-spec devices.
3. Scalability Needs – Government services (like online permits or national IDs) or banking platforms must serve millions of users nationwide. The architecture must support scaling up, or increase its performance, without crashing or slowing down.
4. Data Privacy and Security – Especially for mobile banking or government systems, the architecture must include built-in security protocols to protect sensitive data, complying with laws such as the Data Privacy Act of 2012 (See [Section 4](https://privacy.gov.ph/data-privacy-act/#w4) to see who can be held liable for violating the said act.)
5. Reliability and Fault Tolerance – In educational platforms used in geographically isolated schools, systems should still function or store data temporarily even during power outages or disconnections, then sync once reconnected.
6. Interoperability – Government platforms often need to connect with other systems (e.g., linking local civil registry data to national databases). A well-designed architecture allows smooth integration between different systems.

The architecture is the foundation that determines whether a system can serve users effectively, withstand local challenges, and adapt to future demands—making it especially critical in developing technology solutions that are inclusive, sustainable, and context-aware in your target locale. When doing your milestone, see to it that you’ve considered the users of your system and how it can best adapt to your user’s context.

Just like in FinMark Corporation, it’s likely that applications built in your future workplaces, especially if they are an MSME, have started with a small user base.

However, even if an application starts with a small user base, considering its architecture from the beginning allows developers to make informed decisions and effectively communicate the system's vision. This forward-thinking approach supports better collaboration and prepares the software for future growth or adaptation.

If you think you need more clarity on this topic, you may further read this  [article](https://www.turing.com/blog/software-architecture-patterns-types#_what_is_software_architecture?) .

b. Architectural Patterns

The core principle underlying all software architecture patterns remains the same: to clearly define the essential features of your application, enhance its overall functionality, and increase the efficiency and productivity of the development process.

Before finalizing a specific architecture pattern, it is crucial to thoroughly understand the purpose and characteristics of each one. Choosing an inappropriate architecture can lead to project delays, increased complexity, or even system failure.

To ensure that the selected architecture aligns with your software requirements, develop a strong foundational knowledge of the available patterns and their most suitable use cases by reading the patterns [here](https://docs.google.com/document/d/1kLryhl8mx6LRGJidF5ZB_228pnlbdmUAKijoygBsRMg/view).

If you think you need illustrations for each type for you to better imagine them, refer to this [article](https://medium.com/@the_nick_morgan/what-are-the-10-most-common-software-architecture-patterns-faa4b26e8808) or watch this [video](https://drive.google.com/file/d/1_DeQmZ3--Sc8W14MXL4FHValA_z7lCwx/view?usp=drive_link) (Note that these two alternatives do not contain all of the patterns listed above. They include the most commonly used for specific purposes, so it’s best to also study the table above).

c. Software Architecture Platform Type

Software architecture patterns vary depending on the platform—whether you're building for desktop, mobile, or web. Each platform has different performance needs, user interaction models, and scalability concerns, so the architecture must fit accordingly.

Refer to the tables below to see how each pattern differs depending on the type of platform used. As you read the tables, imagine what you and your groupmates brainstormed about and how the descriptions you thought could fit any of the following use cases [here](https://docs.google.com/document/d/1ukkOcRJTJUVdxLGspdVrd3-8gtyd7YVcCLl1vTjFouk/view).

d. API Architecture

An Application Programming Interface (API) is a clearly defined set of rules and protocols that allows two separate computer systems or software programs to communicate and work with each other. Typically, one system is responsible for publishing the API, while the other accesses or uses it.

The company or developer that publishes the API creates both the interface (the rules or contract that defines how other programs should interact with it) and the implementation (the actual code or program that performs the operations, which is usually hosted on a server). Note that API is a combination of both the rules and the program that carries out those rules.

Think of an API like a Jollibee counter. The menu is like the  interface —it tells you what food items are available and how to ask for them (e.g., by meal number or item name). Behind the counter is the kitchen, which is like the  implementation —it’s where the actual food is prepared after you place your order. You don’t need to know how the food is made; you just follow the menu, and the kitchen does the rest.

According to [Cooksey](https://zapier.com/resources/guides/apis/introduction) (2024), APIs continue to be a demand and will continue to grow:

A 2021 [F5 study](https://www.f5.com/pdf/reports/f5-office-of-the-cto-report-continuous-api-sprawl.pdf) projected that the number of active APIs would grow from under 200 million in 2018 to upwards of 1.7 billion by 2030. Meanwhile, the latest [State of APIs report](https://stateofapis.com/) found that nearly 63% of developers relied on APIs more in 2022 than they had the previous year, and nearly 70% said they expected to rely on them even more in 2023. And on the executive side, over half the CEOs surveyed by Postman's most recent [State of the API Report](https://www.postman.com/state-of-api/outlook-for-apis-and-more/#outlook-for-apis-and-more) anticipated increasing their organizations' investments in APIs in the next year.

APIs allow different applications to share and retrieve information safely and efficiently. They also give developers the freedom to build creative and effective solutions. Creating a well-structured API architecture is a crucial part of developing a successful software application. API architecture refers to the structure and design of how APIs communicate with various parts of a system—such as databases, applications, and external services. It also defines how data is shared between these components, often through HTTP status codes, which act like response signals indicating whether a request was successful, failed, or needs further action.

To simplify, imagine an API like a delivery service in Metro Manila. When you place an order online (the request), the system checks the address, confirms the item, and gives you a response—like “Order Confirmed” or “Address Not Found.” These responses are similar to HTTP status codes (like 200 for OK, or 404 for Not Found) that tell the system what to do next. These codes guide the flow of communication and help systems know how to respond efficiently.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfaW5QaWXTYRysqsmpKd5SeWD5XmU6SQjvxQWY5R1H8l2qoorcUrl5Q-W_IIkBsWsTk6zdyEyglv6fI8FU8ZQzzBphtCskbmTaORRD4us-PuX9x92urGJyeCEn_dJhlE9i3AWJMNg?key=VWz3Q1KHwStmsNnLxBatTTDb)

The most important benefit of API architecture is that it helps developers build applications faster, while ensuring the systems are more stable and reliable. With an API-driven architecture, developers can easily create tools that interact with different systems—without needing to write complicated code for each system or spend too much time fixing errors caused by parameters like [query strings](https://www.claravine.com/a-query-on-using-query-strings-parameters/) (which are small pieces of information passed in a URL).

In short, well-designed API architecture makes software development more efficient—just like how having a reliable logistics network speeds up deliveries across different parts of the country. When well-designed, API architecture ensures that systems are reliable, secure, scalable, and user-friendly, especially in [RESTful APIs](https://www.techtarget.com/searchapparchitecture/definition/RESTful-API) (which are commonly used on the web).

Components of API Architecture

API Architecture is essential in modern web development because it provides a structured framework for creating secure, efficient, and scalable applications. One of the most widely used types of APIs in this architecture is the REST API—short for  Representational State Transfer .

What are REST APIs?

REST APIs are a set of rules and design principles that allow different software systems to communicate over the internet using standard HTTP methods (like GET, POST, PUT, and DELETE). These methods are used to perform actions such as retrieving data, submitting information, updating records, or removing entries from a database.

REST APIs are popular because they are:

1. Stateless – Each request from the client to the server must contain all the information needed to understand and process the request. This means the server does not store information about previous interactions, making the system simpler and more scalable.
2. Resource-based – Data is treated as resources, and each resource is identified by a URL or endpoint (e.g., /users, /products/123).
3. Use standard web protocols – REST relies on existing protocols like HTTP, which makes it easy to implement and integrate with web-based systems.

Why REST APIs Matter in API Architecture

A well-designed API architecture using REST APIs helps developers:

1. Ensure consistent structure across systems
2. Improve security by managing access through authentication and authorization
3. Enhance performance by using caching and efficient data formats like JSON
4. Speed up development by allowing front-end and back-end teams to work independently using clear API contracts

REST APIs adhere to six key principles that guide their design and functionality:

1. Separation—of the client (application requesting data) and the API server (providing data).
2. Statelessness—Each request sent by the client to the server must contain all the information needed for processing.
3. Resource-based—REST APIs present data and functionality as resources accessible through URIs.
4. Uniform Interface—Consistent rules for how clients interact with server resources.
5. Cacheable—Responses to requests should be cacheable by both the client and server as much as possible.
6. Layered system—The architecture allows for the presence of intermediaries like caches and load balancers between servers and clients

REST APIs are preferred because they are easy to scale. More server resources or intermediaries can be added or removed flexibly if client requests increase. Well-defined principles simplify maintenance and upgrades. See the diagram below to visualize the said process:

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf-fR7YFbw0gvPecC4bs49KHy1U9JyiRd1f3oaDwn653mdy_L8XwQsderxVAArXPpBU7ks_9u5KyPb5vOTOJ0LThiqrbACLTwfzKomwOUAKwqSod_ezRxoeZ5kduW--UJgBz8gB-Q?key=VWz3Q1KHwStmsNnLxBatTTDb)

For example, in an e-commerce application:

1. To get product information, a REST API might use:
2. GET /products/123
3. To create a new order:
4. POST /orders
5. To update customer details:
6. PUT /customers/456
7. To delete a product from the inventory:
8. DELETE /products/789

Each of these uses a specific HTTP method and endpoint, following the REST principles. The most commonly used components of an API architecture in REST APIs include the following:

1. Data Model

The data model defines how information is organized and shared between a client (like a mobile app) and a server (like a company’s database). Think of a Lazada shopping app. The data model may define how "Product," "Customer," and "Order" information are structured. For example:

1. Product: name, price, stock
2. Customer: name, contact, address
3. Order: product ID, quantity, payment status

The data model must be flexible and consistent across platforms—whether the user is shopping via the mobile app or website.

2. Endpoints

Endpoints are like specific addresses where you can send or receive data via the API. Each endpoint handles a specific task. Imagine visiting different windows at the LTO (Land Transportation Office) for different services:

1. /register-vehicle : for registration
2. /renew-license : for license renewal
3. In APIs, the app knows which endpoint to go to (URL) to make a specific request—just like you know which LTO window to go to.

A good API has clear and consistent endpoints to ensure smooth communication and proper data exchange.

3. Authentication and Authorization

This ensures that only legitimate users can access the API, and that they can only access what they’re allowed to. At a bank, for example, you need:

1. Authentication: to prove who you are (e.g., ID, ATM PIN)
2. Authorization: to determine what you’re allowed to do (e.g., can view account, but cannot approve loans)

Similarly, APIs may use[ OAuth 2.0 or tokens](https://auth0.com/intro-to-iam/what-is-oauth-2) to control access and ensure data security.

4. SDKs (Software Development Kits)

SDKs are pre-built tools and code libraries that help developers use the API more easily and build applications faster. Think of SDKs like pre-mixed ingredients for baking ube pandesal. Instead of measuring everything from scratch, you get a kit that makes the process faster and error-free. For developers in the Philippines, SDKs make it easier to integrate APIs—like  [GCash’s payment gateway](https://developer.2c2p.com/docs/sdk-method-gcash) —into their mobile apps or websites.

5. Versioning

API versioning allows older and newer versions of the API to exist together, so that existing apps don’t break when updates are made. A telco like Globe Telecom might update its customer portal API (from version 1 to version 2) to include new services. With versioning:

1. Old apps can still use  /v1/account
2. New apps can explore  /v2/account with added features

This ensures [backward compatibility](https://www.techtarget.com/whatis/definition/backward-compatible-backward-compatibility) while encouraging innovation.

6. Analytics and Monitoring

This component helps track API usage, detect issues, and analyze system performance. Like how SM Supermalls monitors foot traffic and store activity, API monitoring tools track which endpoints, or specific URLs or URIs that allow a client to interact with an API resource and perform a specific operation or action, are most used, how fast they respond, and if there are any errors. This helps tech teams improve service quality, avoid downtime, and make informed decisions.

Ensuring Module Compatibility and Scalability

Ensuring module compatibility and scalability is critical in modern software development, especially in systems built with multiple components or services. Compatibility refers to the ability of different software modules or components to work together without conflict. This includes ensuring that modules follow common interfaces, data formats, and communication protocols. Developers must carefully design and test modules to ensure they can interact reliably, whether running in the same environment or distributed across different systems.

Scalability, on the other hand, focuses on a system’s ability to handle increasing workloads or expand in functionality without compromising performance. Scalable modules can manage larger volumes of data, user requests, or system resources by adding more computing power or optimizing code. A scalable design allows applications to grow alongside user demands or business needs without requiring a complete system overhaul. For example, in web applications, developers often implement load balancing, caching, and [microservices ](https://cloud.google.com/learn/what-is-microservices-architecture)architecture to achieve high scalability.

To achieve both compatibility and scalability, developers should adopt standardized practices such as the following:

1. API-driven development
2. API-driven development is an approach where the design and development of Application Programming Interfaces (APIs) are prioritized from the beginning of a project. By first defining how different components will communicate, teams can work in parallel—backend developers build the API implementation, while frontend developers work on the user interface based on the API's specifications.
3. This promotes compatibility, as all modules adhere to clearly defined contracts for data exchange. Additionally, API-first strategies make systems more scalable, since APIs can be reused across multiple platforms (web, mobile, third-party services), and services can be updated independently as long as the API contract remains intact.
4. Modular programming
5. Modular programming is the practice of dividing a software system into smaller, independent, and reusable units called modules. Each module is designed to perform a specific task or function and can be developed, tested, and maintained separately. This promotes compatibility by clearly defining the input/output and dependencies of each module.
6. In terms of scalability, modular systems allow for incremental growth: new features can be added as separate modules, and existing ones can be optimized or replaced without affecting the entire application. This flexibility makes it easier to adapt to changing requirements or growing user bases.
7. Thorough documentation. Testing across different environments and using version control systems also help maintain compatibility as modules evolve. At the same time, designing loosely coupled modules—where each component functions independently—makes it easier to scale individual parts of the system. Ultimately, prioritizing both compatibility and scalability leads to more efficient, adaptable, and future-ready software systems.

See Microsoft’s [Integration Architecture Design](https://learn.microsoft.com/en-us/azure/architecture/integration/integration-start-here?utm_source=chatgpt.com) to learn more about designing integration solutions, including patterns and best practices to ensure compatibility and scalability.

In a nutshell, to build a great platform:

1. You design APIs as communication bridges,
2. Use patterns to ensure solid architecture, and
3. Match your design choices to the platform type you're building.

﻿These three — APIs, patterns, and platform types — form a triangle that supports functionality, scalability, and user engagement in any serious tech platform:

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeWZSW2GuWjJUJFuPecMgVAaTEJwHcST_r1Bbl3HH9u0x9u8C71N1hXN_75STS3a9YDSjQ801dlATaA9f2pAdbNT7OIkiCH9GG9E9BYJ7mgjiTE6uNSDlqTaweUVHk1rAiP7qRlEg?key=VWz3Q1KHwStmsNnLxBatTTDb)

---

# What are the Basic Network and Architecture Design Concepts?

 Now that you’ve explored software architecture—how platforms are structured using modular components, APIs, and design patterns—it’s time to connect those ideas to network architecture and design. A well-designed software system won’t function smoothly if the underlying network can’t support secure, efficient communication between its components. Whether your platform uses microservices, layered modules, or streaming data, each part depends on reliable network connectivity, bandwidth management, and protection from external threats. In this next section, you’ll learn how to design a network infrastructure that matches your software’s architecture, supports its performance needs, and defends it against security risks. If you need a little review on network that is hinged on cybersecurity, watch this video:

<iframe class="ql-video ql-align-center ql-indent-2" frameborder="0" allowfullscreen="" src="https://www.youtube.com/embed/sesacY7Xz3c?si=FOsZDOlPGcPy8xZL" height="364" width="677"></iframe>

Before you can design or secure a networked system, you need to understand how computer networks work at a foundational level. A computer network is a group of interconnected devices—such as computers, servers, and routers—that communicate with each other to share data and resources. Whether it’s a simple local network in a small office or a large-scale infrastructure for an e-commerce platform, the way devices are connected affects everything from speed and reliability to security and scalability. The next paragraphs review network architecture and network topology. If you have encountered these concepts, review the principles [here](https://youtu.be/jq_LZ1RFPfU?si=m8ppTYqbgHaOpx31&t=3), scan the content below, and watch the videos for refresher.

 Network architecture  refers to the overall design and structure of a computer network. It defines how different devices (such as computers, servers, routers, switches, and firewalls) are connected and how data flows between them. Think of it as a blueprint that outlines the components, layout, communication protocols, and rules that determine how information moves within and outside the network.

 Key Elements of Network Architecture:

1. Physical Layout:  The arrangement of devices and cables (topology), such as star, mesh, or bus.
2. Logical Design:  How data travels through the network—this includes addressing, routing, and segmentation.
3. Network Devices:  Components like switches, routers, firewalls, and wireless access points.
4. Protocols:  The “languages” or rules devices use to communicate (e.g., TCP/IP, HTTP, FTP).
5. Security:  Placement of firewalls, access controls, and segmentation to protect data.

 A well-designed network architecture ensures that the system is  efficient, scalable, secure, and reliable  . For example, in a bank or an e-commerce site, the right architecture supports thousands of users, prevents data loss, and keeps hackers out—all while maintaining fast response times. See the following examples:

1. Client-Server Architecture:  Most business systems use this, where user devices (clients) request data from central servers.
2. Cloud-Based Architecture:  Applications and data are hosted in the cloud, accessible from anywhere, allowing easy scaling as the business grows.

 In short, network architecture is the master plan for building and operating a computer network, covering how devices connect, communicate, and stay secure.  Watch this video to review the four basic network architecture types:

<iframe class="ql-video ql-align-center ql-indent-2" frameborder="0" allowfullscreen="" src="https://www.youtube.com/embed/DfdEG4eIdUU?si=5MnbCJCIlkE4whuR" height="315" width="560"></iframe>

 In this next paragraphs, you’ll explore network topologies—the physical and logical layout of how devices are arranged and how data flows between them. Understanding these topologies will help you design smarter, more resilient networks that support the demands of modern platforms.

 Network Topology

 Network topology refers to the arrangement or layout of different elements (such as computers, routers, switches, and cables) in a computer network. It describes how nodes (devices) are physically or logically connected and how data flows within the network.

 Understanding network topology is essential for designing an efficient, scalable, and secure network—especially when you're building your own platform technology (PT) application. The way you structure your network directly affects how smoothly your app can communicate across devices, handle increased user load, and protect sensitive data. Whether you’re managing traffic between microservices, securing API gateways, or ensuring fast response times during peak usage, a well-planned network layout will support the performance and reliability of your platform. In this section, you'll explore network topologies to help you make smart design decisions that align with your app’s functionality and growth.

 How is it important in various contexts?

1. Helps visualize and plan how devices are connected. - A school IT coordinator is setting up a new computer lab with 20 desktop units, a printer, and a Wi-Fi router. Before physically connecting the devices, they draw a network diagram to map where each component will be placed and how they’ll be wired. This planning prevents tangled cables, identifies the best locations for power outlets, and ensures that every device has access to the network without interference. Without this step, they may end up redoing connections, wasting time and resources.
2. Affects the performance, fault tolerance, and scalability of the network. - An online food delivery service is expanding its operations. At first, their system uses a simple shared connection among five tablets used by riders. As orders increase, the network starts to slow down. Realizing the issue, the business shifts to a network layout that supports individual connections with a backup router. Now, if one rider’s device disconnects, others remain functional. They also set it up so more tablets can be added later without disrupting the existing network—improving both performance and scalability.
3. Makes it easier to troubleshoot network issues. - During an exam period, students using school-issued laptops can’t access the shared file server. The IT staff checks the network diagram and sees that all student devices connect through a central switch. By reviewing this layout, they trace the issue to a single malfunctioning port on the switch. Since the topology is clear, the problem is resolved in minutes. If the layout wasn’t documented, the staff might have checked each laptop or cable one by one, delaying the resolution.
4. Guides decisions in network upgrades or expansions. - A local business with a growing customer database decides to add an online ordering system. Before launching, the owner consults the network diagram to assess if the current setup can handle added traffic. The diagram reveals that the router is already managing multiple connected systems—POS, printers, and CCTV. With this information, they invest in a higher-capacity router and segment the network to avoid slowdowns. The network map helped guide these upgrade decisions before problems even surfaced.

 Types of Network Topologies

1. Bus Topology
2. All devices are connected to a single central cable (the "bus").
3. Simple and cost-effective for small networks.
4. If the main cable fails, the whole network goes down.
5. Use case: Small office with minimal devices.

 ![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd8pLzhYQqooSuNoCrQjuI9VT6gTea6dn5AkB0t9m--KdRx-5IoS6i0YX_G7X_FqW3wgjjrxhN2-rex8oNAeG28fouoMJkXUK5YPn__Tb-mbnKCAhdcOX5dc-Op4ObvYV5k7TXSjQ?key=VWz3Q1KHwStmsNnLxBatTTDb)

2. Star Topology
3. All devices connect to a central hub or switch.
4. Easy to manage and isolate issues.
5. If the central hub fails, the entire network is affected.
6. Use case: Most common in homes and schools.

 ![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd5yNawiSx5kxPjz98Bo_hZEwkHCJheyvxuf8W4ZEbJLUxt4j0lTP25pEyJJGkrlplSoGWQbeTk-WOhEVRqUsE88QXt-tImUPHYQbS-ZElX0flSR8U-q7lEm4D8qGUTHS_NyaP5?key=VWz3Q1KHwStmsNnLxBatTTDb)

3. Ring Topology
4. Devices are connected in a circular loop.
5. Data travels in one direction (or both, in dual-ring).
6. A break in the ring can disrupt the network unless redundancy is in place.
7. Use case: Some legacy telecom systems.

 ![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdkS7I8gDlb9m63_DwPdHPcQPgreaPj8n0dYnsoiaj8Um03q8w7f6o2ViTZw7NXAB_ZMb0bN8W2AVBoBSExAInl07iy-_BEYa9IzOYygqA1SvUueMRHyyl7cJU9FMXJvfX0taS2rQ?key=VWz3Q1KHwStmsNnLxBatTTDb)

4. Mesh Topology
5. Every device is connected to every other device.
6. High reliability and fault tolerance.
7. Expensive and complex to set up.
8. Use case: Military networks, advanced enterprise systems.

 ![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXckhgOf8Tc_m1HkhLNLO5Sf4iNSq92RNjWKZSmoKtYjle3leJOrxLYclRW64P2ASL8sMw-_lj86lqmFu82wdeFcQd9mFiOWf4KFBmgLE0ijv5qnyzuh3sUGKgz5G28aNvNCJYNHkw?key=VWz3Q1KHwStmsNnLxBatTTDb)

5. Tree Topology
6. A combination of star and bus topologies.
7. Hierarchical and scalable.
8. Easier to manage but if the main backbone fails, sub-networks are affected.
9. Use case: Large organizations with multiple departments.

 ![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfw_yYZismNs-o9CQ-3z-Sfkj_QrKY1LTvmlDzJy2mlYbJJH27VKT8Zxs47t0xFDxxa3kR4UxGp4mF5k41YSuLtDErlZq1NYROTOLMzuPw_4lk4cTK-jM_RuhISrCFyx6gHri5zUA?key=VWz3Q1KHwStmsNnLxBatTTDb)

6. Hybrid Topology
7. Combines two or more topologies.
8. Offers flexibility and customization.
9. Complexity can make troubleshooting harder.
10. Use case: Enterprise-level networks or data centers.

 ![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcQU3W2cFzRYCWij7Xu6dBt3_-xh3ZK_3EHT8PXowTn8iSgvKoM_NngOGMFUkqIJKKyd5YUMABncXdhsu8k-nzrEHnl0UhbP_Uuq7hEq8tzsib92sx0CJ3HCffk2DNUCSkDogoltA?key=VWz3Q1KHwStmsNnLxBatTTDb)

 Network Diagrams: What Are They?

 A network diagram is a visual representation of a network's structure. It shows how devices are connected and how data flows. Diagrams can be physical (actual cable layouts and hardware locations) or logical (data flow, IP addressing, roles) as seen below:

 Physical:

 ![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeeh0RMGZd5L0AW9VWJJaAE4FARZSGvPKfRcBrj8YBlfMLYc0-L5vjfqyPFii84xDLbAJ1y5TKdTJ7edV9ZFLyMioJVS12lD-AXsr32osHzP6ZZ0uaAMSySt7KDsL4dPFh2NLy7gQ?key=VWz3Q1KHwStmsNnLxBatTTDb)

 [ Image Source](https://images.app.goo.gl/jqEsrK7k9qgCQctZA)

 Logical:

 ![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd5tLUi-CgxgV-ZJpDuwktBQt7KK0F9IepoVBI-b6Ifs27i4-uqZO2Ry4pAKANR3aUPM6tNgFBeDBP0neTFrKlztE7mN3Zhy8Frdfyk68ddPT1JJJ0KDvOMbU34Ihv4xDsj1vkb?key=VWz3Q1KHwStmsNnLxBatTTDb)

 [ Image Source](https://images.app.goo.gl/M5kAsdxzrLYmqZRA8)

 Common Elements in a Network Diagram

1. Nodes: computers, routers, switches, firewalls
2. Connections: lines indicating cables or wireless links
3. Labels: IP addresses, device names, roles (e.g., server, client)

 Diagramming Tools Students Can Use

1. Lucidchart (Free for students):  [ https://www.lucidchart.com/pages/](https://www.lucidchart.com/pages/)
2. Draw.io / Diagrams.net:  [ https://app.diagrams.net/](https://app.diagrams.net/)
3. Cisco Packet Tracer (for simulation):  [ https://www.netacad.com/courses/packet-tracer](https://www.netacad.com/courses/packet-tracer)

 Key Concepts to Remember

1. Physical topology: actual layout of devices and cables.
2. Logical topology: how data flows regardless of physical layout.
3. Scalability: how easily a network can grow.
4. Redundancy: backup paths to avoid single points of failure.
5. Latency and bandwidth: performance factors influenced by design.

 Supplemental Learning Resources

1. [ Cisco’s Introduction to Topologies](https://www.cisco.com/c/en/us/solutions/automation/network-topology.html)  (Beginner-friendly)
2. [ CompTIA Network+ Study Guide on Topologies](https://www.examguides.com/Networkplus/network-plus-2.htm)  (Exam Guide Type)
3. [ YouTube: Network Topology Explained – by PowerCert](https://www.youtube.com/watch?v=tpIctyqH29Q)  (Crash Course)
4. [ IBM’s Overview of Network Diagrams](https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-network-architecture-diagrams&mhsrc=ibmsearch_a&mhq=network+diagram)

 Understanding network topologies and diagrams is like learning the blueprint of how information moves in a digital space. Whether you’re setting up a small home network or studying to be a network engineer, mastering this topic is a foundational skill that will serve you in many areas of IT, cybersecurity, and systems administration.

 Network Performance

 Network performance refers to how well a network handles the data traffic it carries—this includes speed (throughput), responsiveness (latency), reliability (uptime and stability), and packet delivery (no loss or corruption). For platform-based applications—especially those involving real-time transactions, user logins, or API calls—network performance directly impacts user experience and business continuity.

 When network performance is poor, the following are observed:

1. Pages load slowly or not at all
2. Transactions timeout or fail
3. Real-time updates (e.g., dashboards or chat features) lag or disconnect
4. APIs break mid-request, crashing dependent services

 In the context of your PT app, a strong network means faster communication between modules (e.g., frontend ↔ backend ↔ database), efficient API request handling, and smooth load times for users, even during peak hours. High-performing networks also reduce user churn, improve transaction success rates, and support critical services like authentication and payment gateways.

 KPI to Evaluate Network Performance

 This section has been covered by your previous courses. If you feel you need a review, go ahead and read the succeeding parts. Otherwise, skip this section.

 Key metrics used to evaluate network performance include bandwidth, latency, throughput, jitter, and error rate. A high-performing network can reliably transmit large volumes of data quickly and securely, with minimal delays or errors, thereby enhancing productivity and user experience.

 To measure network performance, it's essential to determine key performance indicators (KPIs) such as bandwidth, throughput, latency, jitter, and packet loss. Utilizing network monitoring tools can help in continuously recording the status and statistics of all network elements, proactively identifying issues before they affect service quality. Conducting tests under various conditions assesses how well the network performs under stress or heavy loads. Analyzing the collected data provides insights into the network’s performance, including average speeds, bottlenecks, and error rates. Preparing detailed reports with significant findings assists stakeholders in making informed decisions and planning capacity expansions or upgrades.

 Several factors can impact network performance:

1. Bandwidth: Low bandwidth can affect the speed and efficiency of data transmission. For example, a marketing department might face issues sharing large project files promptly across the network.
2. Latency: High latency can impact synchronous tasks. During a virtual meeting, noticeable conversation delays might occur due to latency.
3. Network Traffic: Excessive traffic can congest a network, hampering performance. For instance, during online registration periods, students might experience slower response times accessing the portal.
4. Physical Infrastructure: Issues in physical equipment, from cables to routers, can hinder performance. In an office, intermittent connectivity might result from a faulty access point or switch.
5. Service Provider: The quality of service from Internet Service Providers (ISPs) significantly influences network efficiency. Upgrading to a dedicated business package or collaborating with a different ISP can ensure consistent and efficient service.

 By identifying and addressing these factors, it's possible to enhance network performance and improve the overall user experience. How do you exactly know that the network performance is good based on the user experience?

 For any network specialist to evaluate network performance, parameters have to be set. While your organizations may also have their own, below are the usual parameters set to gauge whether or not a network built will more or less run smoothly. Any business, be it a sari-sari store going digital, a school offering online classes, or a small online ukay-ukay in Iloilo relying on cloud-based systems, needs an evaluation of its network performance to keep things running smoothly. Refer to the several parameters below to measure and evaluate network performance. Take note that these shall be useful once again when you build your Milestone 2:

1. Bandwidth refers to the maximum amount of data that can be transmitted over a network within a certain time. For example, a municipal hall with only 5 Mbps internet might experience slow performance during peak hours.
2. Throughput is the actual amount of data that successfully gets through the network. In a BPO company in Cebu, low throughput can cause delays in voice calls or screen sharing even if the subscribed bandwidth is high.
3. Latency is the delay that happens when data travels from one point to another. In remote schools using online learning, high latency can make video calls choppy or cause delays when loading modules.
4. Jitter refers to the inconsistency in delay when sending packets of data. This becomes a problem in live streamed classes or meetings, where video or audio can skip or fall out of sync.
5. Packet loss happens when some data doesn’t arrive at its destination. For example, during an online barangay meeting on a weak signal, packet loss might result in missed dialogue or incomplete presentations.
6. Error rate, or bit error rate, tells us how many pieces of data get corrupted during transmission. In a health center using digital records, high error rates might result in inaccurate or delayed patient data transfers.
7. Round-trip time (RTT) is how long it takes for data to go to its destination and back. This is crucial in real-time services like online banking or GCash transactions—high RTT can cause noticeable delays.

 Network availability refers to how often the network is accessible. For private schools in Metro Manila, high availability is essential to keep learning systems online and avoid class disruptions.

 Monitoring Network Performance

 Monitoring your network’s performance is important to make sure everything runs smoothly—whether it’s for a school’s computer lab, a local government office, or a small business. Here’s a simple guide you can use, not just for FinMark, but also for your future workplace.

1. Identify the important metrics. - Before you begin, know which measurements matter most. For example, in an internet café or school in the province, you should pay attention to things like bandwidth usage (how much internet is being used), packet loss (data that doesn’t reach its destination), latency (delay), and jitter (inconsistent delay in video or audio).
2. Choose a network monitoring tool. - There are many tools you can use to monitor your network. Some are free, while others are paid. Popular tools include SolarWinds, PRTG, and Nagios. Just make sure the tool you choose can track the metrics you need.
3. Set up and customize your tool. - Once you’ve picked a tool, install it and connect it to your network. You can customize the tool to focus on the most important metrics. For example, if you're running a school network, set it to alert you if too many users are slowing down the system.
4. Enable alerts. - These tools often let you set alerts. For example, if internet usage gets too high or if latency becomes too long during an online class, the system can notify you right away so you can fix the problem early.
5. Monitor regularly and create reports. - Don’t just check the network once and forget it. Monitor it regularly—daily or weekly—and create reports. This can help you see patterns, like if the internet always slows down after lunch, or if certain users are using up too much bandwidth.
6. Review and adjust. - Your network might grow over time—like when more students use Wi-Fi or when your business adds more computers. Adjust your monitoring settings to match your current needs.

 It’s not enough to build a high-performance network—you need to continuously monitor it to ensure everything stays healthy, especially as your platform grows. Monitoring network performance means actively tracking metrics like bandwidth usage, latency, jitter, packet loss, and device responsiveness. Tools like Wireshark, iftop, Zabbix, or SolarWinds can help visualize these metrics and detect when something goes wrong. You should be more or less familiar with these tools. Otherwise, you’ll use them again in Weeks 6-7.

 In short, monitoring helps in platform development through the following:

1. Early detection of issues: If your login API slows down due to high traffic or a failed route, monitoring tools can alert you before users even notice.
2. Traffic analysis: You can see which services are consuming the most bandwidth and whether certain endpoints are overused or underutilized.
3. Resource planning: Monitoring lets you predict when to scale servers or bandwidth before a crash happens.

 For example, in a local e-commerce app preparing for a flash sale, monitoring reveals that image downloads are hogging bandwidth. As a result, you can prioritize checkout traffic through QoS (Quality of Service) to maintain performance where it matters most.

 Network Scalability

 Network scalability means how well a network can handle more users, devices, or data as a business or organization grows. A scalable network keeps working smoothly even when more people are using it. It doesn’t need to be totally rebuilt when the business expands.

 Why is this important?

 Network scalability is the ability of your network to handle increased demand without a drop in performance. As your platform gets more users, processes more data, or supports more features, your network must scale in response. This could mean adding more servers, optimizing routing paths, upgrading bandwidth, or redesigning your network layout. Consider the two scenarios below:

1. A small online seller in Bacolod starts by using a basic Wi-Fi connection at home to manage her orders. This setup works well while the business is small and she's the only one using the network. As her business grows, she decides to open a physical store, hire staff, and use more devices like computers, printers, and a point-of-sale system. To keep everything running smoothly, she upgrades her network to one that can handle more users and devices without slowing down. This shows scalability because the network is able to adjust and grow along with the needs of the business. Instead of experiencing lag or breakdowns, the upgraded network supports the increased demand, showing that it can scale effectively as the business expands.
2. A school in Quezon City initially has only 50 students using its internet connection for research and online activities. At this stage, the existing network is more than enough to support their needs. However, as the years pass, the student population increases to 300, and the school starts offering more online classes, digital learning tools, and even administrative systems that rely on the internet. To keep up with these changes, the school upgrades its network infrastructure—adding more bandwidth, better routers, and additional access points. This shows scalability because the school’s network was able to grow and adapt to meet the rising demand. The system remains fast and reliable despite the increase in users and online activities, proving it can scale with the school's development.

 Simply put, scalability makes sure the network grows with the needs of the users. Without scalability:

1. Your platform may crash during traffic spikes (e.g., registration surges or promotional campaigns)
2. Delays in API responses could stack up, causing service degradation
3. You’ll struggle to onboard more users or offer new features without breaking existing systems

 Technologies that Support Scalability

 As businesses, schools, and government offices grow and adopt more digital services, the network infrastructure must also scale to meet increasing demands. Several modern networking technologies help ensure that networks remain flexible, efficient, and reliable even as they expand. Below are key technologies that support network scalability:

1. AIOps (Artificial Intelligence for IT Operations) - AIOps combines artificial intelligence and machine learning with traditional IT operations to improve the way networks are managed. It automates tasks like network monitoring, issue detection, and troubleshooting. Instead of waiting for a network problem to cause delays—like during an online enrollment rush or a product launch—AIOps can detect unusual patterns early and suggest or implement fixes. It also helps predict future network demands using data, allowing companies to prepare in advance. This makes the network smarter, faster, and more adaptable.
2. AI Networking - AI networking is closely related to AIOps but focuses more directly on improving network performance using artificial intelligence. It helps automatically balance internet traffic, manage congestion, and fix issues without human intervention. For example, in a BPO company with hundreds of simultaneous calls and data transfers, AI networking ensures voice quality remains high by adapting to traffic conditions in real time. This is key in large-scale or rapidly growing organizations where manual adjustments aren’t fast enough.
3. Cloud-Native Network Architectures - Cloud-native architectures are designed specifically to work in cloud environments, where computing resources like storage, applications, and network components are accessed online rather than through physical hardware. In this model, businesses can scale up or down easily—such as during peak sales periods in e-commerce or during nationwide exams in schools using online testing systems. Since the network runs through cloud infrastructure, companies only pay for what they use and can adjust instantly, without expensive upgrades or downtime.
4. Software-Defined Networking (SDN) - SDN separates the control of the network (the “brain”) from the physical devices like routers and switches (the “body”). This makes the network easier to manage, program, and scale. For example, if a company in Manila opens a new branch in Davao, SDN allows IT staff to remotely adjust and scale the network settings without needing to be physically present. SDN also supports faster integration of new services or apps, helping organizations stay agile and responsive to market demands.

 All these technologies support scalability by making the network more intelligent, flexible, and responsive. Whether you’re operating a growing retail chain, expanding a university's online learning platform, or digitizing government services, these networking tools ensure that you don’t have to overhaul your system every time your demand increases. Instead, your network can grow with you—quickly, efficiently, and without disruption.

 While a network that is scalable is beneficial to businesses, its reliability also has to be established. Network reliability is well defined in [ Journal of Network and Computer Applications, 2018](https://www.sciencedirect.com/science/article/pii/S108480451730406X)  :

 [N]etwork reliability is a fundamental requirement in realtime (sic) applications where time-sensitive content is delivered to multiple receivers simultaneously using  [ network multicast](https://www.sciencedirect.com/topics/computer-science/multicast-network)  functionalities. The reliable delivery of this content depends on the existence of a fault resilient network that is able to restore  [ network services](https://www.sciencedirect.com/topics/computer-science/network-service)  in case of a failure. Failure protection can be implemented using either local protection schemes such as fast reroute or end-to- end schemes such as redundant trees.

 Network Security Tools

 Firewalls, VPNs, IDs are all network security tools, but they serve different roles and often work together to protect a network from threats. Imagine this: Your network is like your house in a typical subdivision. The firewall is the guard at the village gate, checking who enters and making sure only visitors with proper ID or stickers are allowed through. The VPN is like your private car with heavily tinted windows, safely driving your family through busy city roads so outsiders can’t see where you're going or what you’re carrying. The IDS (Intrusion Detection System) is the CCTV installed around your house, quietly monitoring what’s going on inside—ready to alert you if someone tries to break in or do anything suspicious. See how all of them secure your house, your network, but they do it in different capacities. In short, these three perform the following:

1. They Protect Different Layers of Security
2. Firewalls act as gatekeepers. They decide which traffic is allowed or blocked, based on rules. They're your first line of defense.
3. VPNs ensure secure communication over public or untrusted networks. They encrypt data, making it unreadable to hackers.
4. IDS are watchdogs. They monitor what’s happening after traffic enters the network. They detect suspicious behavior and alert admins.
5. They Work Together to Form a Layered Defense
6. The firewall blocks unauthorized traffic (like hackers trying to enter).
7. A remote employee uses a VPN to securely access company files from home.
8. Meanwhile, the IDS is watching the whole network and notices unusual activity, like too many failed logins—so it alerts the IT team.
9. They Share and Complement Data
10. Some modern systems integrate firewalls and IDS, so when the IDS detects something unusual, the firewall can automatically block it.
11. IDS tools also monitor VPN traffic to ensure encrypted data isn’t hiding malicious activity.

 If you think you need more discussion on this, refer to the section below:

#### What is a Firewall?

 A firewall is a security device—either software-based, hardware-based, or both—that monitors and controls incoming and outgoing network traffic based on a set of security rules.

#### Types of Firewalls:

1. Packet-filtering firewalls – Analyze packets and allow/block them based on source/destination IPs, ports, and protocols.
2. Stateful inspection firewalls – Monitor active connections and decide which network packets to allow through.
3. Application-level firewalls (Proxy firewalls) – Work at the application layer, filtering traffic for specific applications like HTTP, FTP, etc.
4. Next-Generation Firewalls (NGFWs) – Combine traditional firewall functionality with advanced features like deep packet inspection, intrusion prevention, and more.

 A university in Manila sets up a firewall to block access to social media sites during school hours and protect its student database from external attacks.

#### What is a VPN?

 A VPN creates a secure and encrypted tunnel between a user’s device and a remote server, allowing safe and private browsing, especially when using public or untrusted networks.

#### Key Benefits:

1. Protects data from eavesdropping
2. Masks IP address and location
3. Secures remote work or off-site access
4. Bypasses content filters or geo-blocking

#### Types of VPNs:

1. Remote Access VPN – Commonly used by remote employees to access corporate networks securely.
2. Site-to-Site VPN – Connects two networks securely, such as two office branches in different cities.

 A call center agent working from home in Cavite uses a VPN to securely connect to the company’s main server located in Makati.

#### What is an IDS?

 An Intrusion Detection System (IDS) is a tool that monitors network traffic for suspicious activity and potential threats, alerting administrators when something unusual is detected.

#### Types of IDS:

1. Network-based IDS (NIDS) – Monitors traffic across the entire network.
2. Host-based IDS (HIDS) – Installed on individual devices to monitor system logs and activities.

#### IDS vs. IPS (Intrusion Prevention System):

1. IDS detects and alerts but does not take direct action.
2. IPS detects and blocks the threat automatically.

 A government office uses IDS to detect unauthorized access attempts during off-hours and sends real-time alerts to the IT security team.

 Using firewalls, VPNs, and IDS together is an example of a layered security strategy—also known as defense in depth. Each tool addresses different types of threats:

1. The firewall blocks unwanted traffic.
2. The VPN ensures safe communication over the internet.
3. The IDS watches for unusual activity and alerts admins.

 This layered security approach significantly increases protection by addressing different types of threats at multiple points within the network. Each layer adds a level of defense that compensates for potential weaknesses in the others. This is particularly important for organizations handling sensitive data—such as schools, which store student records and grades; banks, which manage financial transactions and personal client information; and healthcare providers, which maintain confidential patient records and medical histories. In these environments, a single breach could result in serious consequences such as identity theft, financial loss, or violations of data privacy laws like the Data Privacy Act of 2012 in the Philippines. By combining firewalls, VPNs, and intrusion detection systems (IDS), these institutions can ensure that data is protected not just from external attacks, but also from internal threats and accidental exposure. This multi-layered defense system helps them maintain trust, compliance, and operational continuity in an increasingly digital landscape.

 Cybersecurity Frameworks and Compliance Standards

 It cannot be overstated that in today's hyper-connected world, data is one of the most valuable assets—and also one of the most vulnerable. From banks and schools to hospitals and government agencies, all types of organizations must adopt strong cybersecurity practices to protect sensitive information from breaches, theft, or misuse. This is where cybersecurity frameworks and compliance standards come in. These serve as structured guidelines to help organizations identify risks, implement security measures, and maintain trust among stakeholders.

 What Are Cybersecurity Frameworks?

 A cybersecurity framework is a structured set of best practices, policies, and procedures that help an organization manage and reduce cybersecurity risk. Think of it as a blueprint for building a secure digital environment. Frameworks are not laws, but they provide widely accepted guidance that can align with legal and regulatory requirements.

#### Key Benefits:

1. Standardizes how organizations assess and address cyber risks.
2. Encourages continuous improvement in cybersecurity posture.
3. Builds trust with customers, investors, and regulatory bodies.

 Major Cybersecurity Frameworks Used Globally

#### 1. NIST Cybersecurity Framework (USA)

 Developed by the U.S. National Institute of Standards and Technology, the NIST CSF is one of the most widely adopted frameworks, even outside the U.S.

 Core Functions:

1. Identify – Understand assets, systems, and risks.
2. Protect – Implement safeguards like firewalls, training, and access control.
3. Detect – Monitor and identify anomalies or attacks.
4. Respond – Contain the impact of an incident.
5. Recover – Restore services and update security policies.

#### 2. ISO/IEC 27001

 An international standard for managing information security. It provides a systematic approach to managing sensitive information and includes physical, technical, and legal controls.

 Focus Areas:

1. Risk management
2. Access control
3. Security policy development
4. Incident response

#### 3. CIS Controls (Center for Internet Security)

 A prioritized list of security actions aimed at stopping the most common and damaging cyberattacks. Ideal for small to medium-sized organizations looking for a practical starting point.

#### 4. COBIT (Control Objectives for Information and Related Technologies)

 Used primarily for IT governance and aligning IT goals with business objectives. It includes cybersecurity as part of a broader IT management strategy.

 Compliance Standards and Regulations

 While frameworks are voluntary, compliance standards are mandatory rules or laws that organizations must follow—especially in regulated industries like healthcare, finance, and education.

#### 1. Data Privacy Act of 2012 (Philippines)

1. Overseen by the National Privacy Commission (NPC)
2. Protects personal data in both public and private sectors.
3. Organizations must appoint a Data Protection Officer (DPO) and adopt privacy measures.
4. Requires breach notification within 72 hours of discovery.

#### 2. General Data Protection Regulation (GDPR - EU)

1. Applies to organizations handling the personal data of EU citizens.
2. Sets strict rules on data collection, processing, and consent.

#### 3. HIPAA (USA – Health Insurance Portability and Accountability Act)

1. Applies to healthcare providers, insurers, and business associates.
2. Focuses on protecting health records and patient privacy.

#### 4. PCI DSS (Payment Card Industry Data Security Standard)

1. Applies to any business that processes, stores, or transmits credit card information.
2. Requires secure systems, regular vulnerability scans, and strict access controls.

 How Frameworks and Compliance Work Together

 Think of frameworks as the strategy and compliance standards as the rules. Many organizations use frameworks like NIST or ISO/IEC 27001 to achieve or maintain compliance with laws like the Data Privacy Act or GDPR.

 Example:

 A school in the Philippines may follow NIST’s “Protect” function to strengthen access control, while also complying with the NPC’s requirement to safeguard student information under the Data Privacy Act.

 Challenges in Implementation

1. Resource Constraints: Small businesses may lack budget or skilled IT staff.
2. Evolving Threats: Cyber threats change constantly, requiring continuous updates.
3. Compliance Overload: Organizations operating internationally may need to comply with multiple overlapping regulations.

 Best Practices for Adoption

1. Conduct Risk Assessments – Identify assets, vulnerabilities, and potential impact.
2. Adopt a Framework – Choose one that fits your industry and size.
3. Train Your Team – Human error is still one of the biggest security risks.
4. Update Regularly – Stay current with both threats and legal requirements.
5. Document Everything – In audits or breach investigations, detailed records matter.

 Cybersecurity frameworks and compliance standards form the foundation of responsible digital practices. While frameworks offer the "how", compliance standards provide the "must." Together, they empower organizations to be more secure, more resilient, and more trusted in today’s digital world. Whether you’re managing a public school’s database, leading a startup’s cloud infrastructure, or overseeing a hospital’s IT system, adopting a structured cybersecurity approach is no longer optional—it’s essential.

 If you feel you need a more advanced reference regarding Network, do check out the following references below:

1. [ Cloud VPN](https://cloud.google.com/network-connectivity/docs/vpn/concepts/best-practices)
2. [ Principles of Zero Trust](https://www.ncsc.gov.uk/collection/zero-trust-architecture/introduction-to-zero-trust)
3. [ Network Functions Virtualization](https://www.ibm.com/think/topics/network-functions-virtualization)
4. [ Advanced Persistent Threats](https://www.ibm.com/think/topics/advanced-persistent-threats)
5. [ Cryptographic Attacks](https://www.packetlabs.net/posts/cryptography-attacks/)

---

# What are basic concepts for data architecture?

When you build a platform, data doesn’t just live in a database—it travels. It moves between services, across networks, to and from clients, and into storage or analytics systems. That’s why your data architecture (how data is organized, validated, processed, and stored) must align closely with your network architecture (how devices, servers, and services are connected and secured).

#### 1. Data Flow Depends on Network Routing

Data pipelines move information between sources and destinations: for example, user data from the frontend to the backend, or cleaned transaction data to a reporting server. These flows rely on network routing to reach their destination efficiently. If the network is misconfigured, slow, or overloaded, your data pipelines may break or introduce delay.

Example: In a local delivery app, customer order data is sent from the app to the backend and then to the dispatch center’s dashboard. If the network path isn’t optimized, delivery data could arrive late, affecting logistics.

#### 2. Data Integrity and Privacy Need Network Security

Even the best data architecture can't protect data in transit without strong network security. That’s where VPNs, firewalls, and encryption protocols come in. Sensitive data like user credentials, financial records, or medical information must travel through secure, encrypted channels, and your network must be designed to protect these flows.

Example: A payment gateway might encrypt transaction data at the app level, but it also needs a secure VPN tunnel and firewall rules that block any external access to the API that handles payments.

#### 3. High-Volume Data Requires Scalable Network Design

As your platform scales and handles more data—such as logs, transactions, or user activity—your network must handle the load. This means increasing bandwidth, optimizing routes, or using[ content delivery networks ](https://www.ibm.com/think/topics/content-delivery-networks#:~:text=Senior%20Editorial%20Strategist-,What%20is%20a%20CDN?,example%2C%20live%20video%20feeds).)(CDNs) and caching to reduce traffic spikes. Without a scalable network, even a well-designed data pipeline can become a bottleneck.

Example: An analytics platform that collects real-time data from hundreds of IoT sensors must have a network that supports continuous streaming without latency or dropouts.

#### 4. Real-Time Data Needs Reliable Connectivity

If your platform uses real-time features—like live dashboards, alerts, or messaging—your data architecture must be paired with a low-latency network. Even a small delay in data delivery can affect the accuracy of reports or user interactions.

Example: A traffic monitoring app for a city government must show congestion data within seconds. If the network is unstable, even real-time data pipelines will become useless.

Together, your data and network layers must be designed to support one another. Without this alignment, your platform risks delays, errors, or even data breaches. But when they’re well-connected, they enable a seamless, secure, and scalable digital experience.

Data Pipelines

A Data Flow Diagram (DFD) is a visual tool that maps out how data moves through a system. It illustrates the flow of information between processes, data stores, and external entities, providing a clear picture of how data is handled within a system.

Key Components of a DFD:

1. Processes: Represent operations or functions where data is processed.
2. Data Stores: Depict where data is stored within the system.
3. External Entities: Show sources or destinations of data outside the system.
4. Data Flows: Arrows that indicate the direction and movement of data between components.![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfJrds3pzPKq7tFRtORticZFv6aOFKogNHMpYAJpx0ZewYvjAWy9ALbdpwuLZEpowpgXHPbAo0K_4jdiPJjfmtKj0UYWR5_942RHscHmlnYPK2RJ91yVttZFkyOi-oArq2jEcwN?key=VWz3Q1KHwStmsNnLxBatTTDb)

[Image Source](https://hazelcast.com/foundations/event-driven-architecture/data-pipeline/)

Types of DFDs:

A Logical Data Flow Diagram (DFD) shows what happens in the business—it focuses on the activities, events, and the data needed for each step. It doesn’t include any technical details like hardware or software.

A Physical DFD, on the other hand, shows how the system will work in real life. It includes things like servers, databases, paper forms, and the people or systems involved in handling data.

Together, logical and physical DFDs help you understand both the business process and the technical implementation. You can use them to visualize the current system and plan how to improve it in your data pipeline design.

1. Logical DFDs: Focus on the business and how data flows through the system, without delving into technical details.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfghslmIQxnS8QJslsFLO3lvp1LQZ5ByggWHNRMBI1_yY77pIsn8nARbe-dEwItVQXVPX4Jos4_k15xtr7_vqnJ06vHGRbN-BY_KiAUR3LFOWkRoECns4blCcCSA2wUR9z_ZfH_1Q?key=VWz3Q1KHwStmsNnLxBatTTDb)

1. Physical DFDs: Detail how the system is implemented, including hardware, software, and file structures.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdjMz2Ma8OCzwSLRPVNQYDXehHz0TP2VnRAS2ldjpWfpU8R1VrSL0b61i3FtrW5CWq0D2JLDldthmmrGOSPSf-0zSgwd_Or1nqsD7BlVBtRY6sJKu5vlkcNZttb50WfAJvr0xoc?key=VWz3Q1KHwStmsNnLxBatTTDb)

[Image Source](https://www.visual-paradigm.com/guide/data-flow-diagram/logical-vs-physical-data-flow-diagrams/#:~:text=Data%20flow%20diagrams%20(DFDs)%20are,It%20s%20how%20business%20controls.)

How do you use this for your PT?

Imagine you're building a platform technology for a company’s HR department that still uses outdated, manual processes to manage job applications. Instead of jumping straight into coding or choosing a tool, you start by creating a logical data flow diagram (DFD). This diagram focuses on the business activities—not the technology. You map out what the HR team does: creating job posts, publishing them, receiving applications, entering candidate info, notifying hiring managers, updating candidate status, and sending updates to applicants. This logical flow gives a clear picture of how information moves through the recruitment process from a business perspective.

Next, you use this logical DFD to propose a better experience using your platform. For example, your new logical flow might include automatic alerts to hiring managers when new applicants meet job requirements, or instant access to applicant summaries and comparison tools inside the platform. This step helps define what functionalities your platform should deliver to support real business needs.

Finally, you create a physical DFD that shows how these improvements will be built and run within your platform technology. This version includes the technical elements—such as the user interface for HR staff, cloud-based databases to store applicant data, APIs for job board integration, and notification services for updates. You can even map out how different software stacks (e.g., Firebase vs. Supabase, or Node.js vs. Django) might implement the same process. This physical DFD helps your team assess and decide on the best setup for performance, scalability, and security—ensuring the platform delivers both business value and technical robustness.

Creating a DFD:

1. Identify the major inputs and outputs of the system.
2. Develop a Level 0 DFD to represent the system's context.
3. Break down the main process into sub-processes for a Level 1 DFD.
4. Continue decomposing processes into more detailed levels as necessary.
5. Validate the DFD to ensure accuracy and completeness.

Utilizing DFDs in the data pipeline module helps in understanding and designing efficient data processing systems, ensuring clarity in how data is handled from input to output.

Data Architecture

Data architecture refers to how data is collected, stored, organized, and accessed in a system or organization. Think of it like building a house: before building, you need a floor plan. In the same way, companies need a data plan—a blueprint for how data flows from one place to another.

To illustrate, consider a fast-food chain in the Philippines—let’s say Jollibee or a similar local franchise—uses multiple systems in its daily operations:

1. Online Ordering System – for customers ordering through the mobile app or website.
2. Point-of-Sale (POS) System – used in physical branches for dine-in and take-out transactions.
3. Delivery Tracking System – to monitor where the food is, who the rider is, and when it will arrive.

These systems collect different kinds of data, at different times, and in different formats. Here's where data architecture and data pipelines come in:

1. Data architecture acts like the overall blueprint that defines how data flows and connects across these systems. It ensures:
2. All systems are designed to send data to a central location (like a cloud database).
3. Data formats are compatible and standardized (e.g., same time zone, currency, product codes).
4. There's a structure that supports easy retrieval, real-time syncing, and future upgrades.

In this case, data architecture ensures that online, in-store, and delivery data are not isolated or siloed—it connects everything into one intelligent ecosystem.

1. Once the systems are connected, data pipelines serve as the delivery trucks that move data from each system to the central data storage or dashboard in real-time.
2. The pipeline takes in sales from POS every minute.
3. It pulls online order updates from the app.
4. It fetches rider status updates from the delivery system.

Before the data is displayed in management dashboards, the pipeline may also clean and transform it—like correcting duplicate orders, converting all timestamps to Philippine Standard Time, or grouping sales by product type. Without a well-designed data pipeline, management might see delayed or inaccurate data—like thinking Chickenjoy is out of stock when it's not, or missing sales spikes during lunch hours.

1. Thanks to data architecture (the structure) and data pipelines (the flow), the fast-food chain’s management can make real-time decisions like:
2. Restocking popular items before they run out.
3. Redirecting delivery staff to high-demand areas.
4. Identifying underperforming branches quickly.

Clearly, data architecture gives you a high-level view or blueprint of how all the parts of a data system work together—like a map that shows how data flows, where it is stored, and how it is accessed and used by different users or systems. It helps answer questions like: Where does the data come from?; Where is it stored?; How is it processed?; and Who uses it and how?

A solid data architecture brings together various data management platforms and storage systems, including:

1. Databases – For structured, day-to-day data (e.g., MySQL, PostgreSQL).
2. Data Warehouses – For storing large volumes of historical, cleaned, and structured data used for analysis (e.g., Amazon Redshift, Snowflake).
3. Data Lakes – For storing raw or unstructured data at scale (e.g., logs, images, videos), often used in big data environments.
4. Data Marts – Smaller, more specific subsets of a data warehouse, often focused on a particular department (e.g., Sales data mart).
5. Cloud storage and real-time data streams – For modern web-based systems and IoT.

Each of these tools serves a different purpose, but when planned correctly, they all connect through the data architecture to support the full data lifecycle—from collection to storage, transformation, and usage.

As organizations grow and handle more complex data, newer types of data architectures are becoming popular:

#### 1. Data Fabric

A data fabric acts like a smart layer that sits across all your data systems—whether on-premise or in the cloud—and allows you to connect, access, and manage data more easily from a central point. It helps automate data integration, ensure consistency, and reduce redundancy.

Think of it like a giant, intelligent spider web that links all your data sources together.

#### 2. Data Mesh

A data mesh shifts the focus from centralizing data to treating data as a product owned by different teams. Instead of relying on a single data team to handle everything, each department (like HR, Marketing, or Sales) takes responsibility for their own data products. These teams manage, clean, and share their data through standard APIs, making it easier for others to access and reuse.

This approach promotes:

1. Decentralization (no bottleneck at a central data team)
2. Metadata standardization (everyone describes and labels their data in a consistent way)
3. Data democratization (any authorized team or system can access useful data across the company)

When businesses, institutions, or even government agencies build good data architecture:

1. They reduce duplication of effort and data.
2. They enable real-time decision-making.
3. They improve data security, access, and trustworthiness.

To put all these in perspective, consider a financial technology company, GCash.

In financial technology (fintech), such as GCash, data is the backbone of everything—from mobile wallet transactions and loan processing to fraud detection and personalized banking services. A solid data architecture ensures that all systems involved in collecting, storing, analyzing, and securing financial data work seamlessly together. Its data architecture connects everything from customer transaction records, Loan application data, KYC (Know Your Customer) documents,

Credit scoring algorithms to real-time fraud detection systems.

Based on these, GCash’s data architecture may have the following components:

1. Databases – Store structured data such as user profiles, bank account details, and balances.
2. Data Warehouses – Hold historical financial data used for trends, reporting, and forecasting (e.g., daily transaction summaries).
3. Data Lakes – Collect unstructured data like log files, clickstreams, or scanned ID photos from KYC verification.
4. Real-Time Processing Engines – Needed for services like instant payments, mobile money transfers, or fraud detection alerts.
5. APIs – Used for integrating with partner banks, e-commerce sites, and payment gateways.

Maybe, they may even have the modern ones:

#### Data Fabric

A data fabric enables seamless data integration across multiple systems—on-premises and in the cloud. For example, since Gcash offers loans, insurance, and payment services, a data fabric ensures that customer data is consistent across all platforms. It also enables real-time analytics, helping it offer instant credit scoring or show real-time spending insights to users. Check out your GScore which is its basis for your access to its services. Gcash benefits from data fabric as it Improves data consistency and compliance while enabling fast services.

1. Data Mesh

A data mesh decentralizes data ownership. Instead of one central data team managing everything, each product or department—like the payments team, lending team, or fraud detection team—owns and manages its own “data products.” They clean, prepare, and expose their data through standardized APIs, so other teams (or even external partners) can reuse it. The good thing about data mesh is that it scales well as the company grows, encourages accountability, and speeds up product development.

To have a more comprehensive understanding of data management systems, refer to the excerpt from IBM’s [What is Data Architecture](https://www.ibm.com/think/topics/data-architecture):

Types of data management systems

#### Data warehouses

A [data warehouse](https://www.ibm.com/topics/data-warehouse) aggregates data from different relational data sources across an enterprise into a single, central, consistent repository. After extraction, the data flows through an [extract, transform and load (ETL)](https://www.ibm.com/topics/etl) data pipeline, undergoing various data transformations to meet the predefined data model. When it loads into the data warehousing system, the data lives to support different business intelligence (BI) and data science applications.

#### Data marts

A [data mart](https://www.ibm.com/topics/data-mart) is a focused version of a data warehouse that contains a smaller subset of data important to and needed by a single team or a select group of stakeholders, such as the HR department. Because they contain a smaller subset of data, data marts enable a department or business line to discover more focused insights more quickly than possible when working with the broader data warehouse data set.

Data marts originally emerged in response to the difficulties organizations had setting up data warehouses in the 1990s. Integrating data from across the organization at that time required numerous manual coding efforts and was impractically time-consuming. The more limited scope of data marts made them simpler and faster to implement than centralized data warehouses.

#### Data lakes

While data warehouses store processed data, a [data lake](https://www.ibm.com/topics/data-lake) houses raw data, typically petabytes of it. A data lake can store both structured and unstructured data, which makes it unique from other data repositories. This flexibility in storage requirements is useful for data analysts, data scientists, data engineers and developers, enabling them to access data for data discovery exercises and [machine learning (ML)](https://www.ibm.com/topics/machine-learning) projects.

Data lakes were originally created as a response to the data warehouse’s failure to handle the growing volume, velocity and variety of big data. While data lakes are slower than data warehouses, they are also cheaper as there is little to no data preparation before ingestion. Today, they continue to evolve as part of data migration efforts to the cloud.

Data lakes support a wide range of use cases because the business goals for the data do not need to be defined at the time of data collection. However, 2 primary use cases include data science exploration and data backup and recovery efforts.

Data scientists can use data lakes for proofs-of-concept. Machine learning applications benefit from the ability to store structured and unstructured data in the same place, which is not possible using a relational database system.

Data lakes can also be used to test and develop big data analytics projects. When the application has been developed and the useful data has been identified, the data can be exported into a data warehouse for operational use, and automation can be used to make the application scale.

Data lakes can also be used for data backup and recovery, due to their ability to scale at a low cost. For the same reasons, data lakes are good for storing “just in case” data for which business needs have not yet been defined. Storing the data now means it is available later as new initiatives emerge.

#### Data lakehouses

A [data lakehouse](https://www.ibm.com/topics/data-lakehouse) is a [data platform](https://www.ibm.com/topics/data-platform) that merges aspects of data warehouses and data lakes into one [data management ](https://www.ibm.com/data-management)solution.

A lakehouse combines low-cost storage with a high-performance query engine and intelligent metadata governance. This enables organizations to store large amounts of structured and unstructured data and easily use that data for AI, ML and analytics efforts.

#### Databases

A [database](https://www.ibm.com/think/topics/database) is the basic digital repository for storing, managing and securing data. Different types of databases store data in different ways. For example, [relational databases ](https://www.ibm.com/topics/relational-databases)(also called "[SQL](https://www.ibm.com/think/topics/structured-query-language) databases") store data in defined tables with rows and columns. Nonrelational databases (also called "[NoSQL databases](https://www.ibm.com/topics/nosql-databases)") can store it as various data structures, including key-value pairs or graphs.

Types of data architectures

#### Data fabrics

A [data fabric](https://www.ibm.com/topics/data-fabric) is an architecture that focuses on the automation of [data integration](https://www.ibm.com/topics/data-integration), data engineering and [governance](https://www.ibm.com/topics/data-governance) in a data value chain between data providers and data consumers.

A data fabric is based on the notion of “active metadata” that uses [data catalogs](https://www.ibm.com/topics/data-catalog), knowledge graphs, semantics, data mining and machine learning technology to discover patterns in various types of [metadata](https://www.ibm.com/think/topics/metadata) (for example, system logs, social and more). Then, it applies this insight to automate and orchestrate the data value chain.

For example, a data fabric can enable a data consumer to find a data product and then have that data product provisioned to them automatically. The increased data access between data products and data consumers leads to a reduction in data siloes and provides a more complete picture of the organization’s data.

Data fabrics are an emerging technology with enormous potential. They can be used to enhance customer profiling, [fraud detection](https://www.ibm.com/topics/fraud-detection) and preventive maintenance.

According to Gartner, data fabrics reduce integration design time by 30%, deployment time by 30% and maintenance by 70%.

#### Data meshes

A [data mesh](https://www.ibm.com/topics/data-mesh) is a decentralized data architecture that organizes data by business domain.

Using a data mesh, the organization needs to stop thinking of data as a by-product of a process and start thinking of it as a product in its own right. Data producers act as data product owners. As subject matter experts, data producers can use their understanding of the data’s primary consumers to design APIs for them. These APIs can also be accessed from other parts of the organization, providing broader access to managed data.

More traditional storage systems, such as data lakes and data warehouses, can be used as multiple decentralized data repositories to realize a data mesh. A data mesh can also work with a data fabric, with the data fabric’s automation enabling new data products to be created more quickly or enforcing global governance.

As organizations build their roadmaps for tomorrow’s applications, including AI, [blockchain](https://www.ibm.com/topics/blockchain) and Internet of Things (IoT) workloads, they need a modern data architecture that can support the data requirements.

The top characteristics of a modern data architecture are:

1. [Cloud-native](https://www.ibm.com/topics/cloud-native) and cloud-enabled so that the data architecture can benefit from the elastic scaling and high availability of the cloud.
2. Robust, scalable and portable data pipelines, which combine intelligent workflows, cognitive analytics and real-time integration in a single framework.
3. Seamless data integration, using standard API interfaces to connect to legacy applications.
4. Real-time data enablement, including validation, classification, management and governance.
5. Decoupled and extensible, so there are no dependencies between services, and open standards enable interoperability.
6. Based on common data domains, events and microservices.
7. Optimized to balance cost and simplicity.

Designing Scalable Data Pipelines

Whether your architecture is the latest or the classic one, its scalability is important for a whole system to function well. Think of data architecture as the overall design and blueprint of how an organization’s data is structured, stored, and accessed—while data pipelines are the routes that move data from one part of that structure to another. You can’t build an efficient, reliable data pipeline without a strong data architecture in place—just like you can’t build reliable plumbing in a building without a proper blueprint of the building’s layout.

A data pipeline is like a delivery truck that carries data from one point to another. A scalable pipeline means that even if the data volume increases—like during Lazada’s 11.11 sale—the system can handle the extra load smoothly without crashing or slowing down.

Key Concepts:

1. Ingestion – Collecting data (e.g., customer orders)
2. Transformation – Cleaning or organizing data (e.g., correcting misspelled barangay names)
3. Storage – Saving it in databases or data lakes
4. Output – Making data available for analysis

An online learning platform used by a university stores thousands of student interactions every day. During finals week, usage spikes. A scalable pipeline ensures student data is collected and processed in real-time, even when thousands log in at once.

Types of Data Pipelines

[IBM ](https://www.ibm.com/think/topics/data-pipeline)lists the common data pipelines and their appropriate application:

Batch processing

The development of batch processing was a critical step in building data infrastructures that were reliable and scalable. In 2004, [MapReduce](https://www.ibm.com/topics/mapreduce), a batch processing algorithm, was patented and then subsequently integrated into open-source systems, such as [Hadoop](https://www.ibm.com/topics/hadoop), CouchDB and MongoDB.

As the name implies, batch processing loads “batches” of data into a repository during set time intervals, which are typically scheduled during off-peak business hours. This way, other workloads aren’t impacted as batch processing jobs tend to work with large volumes of data, which can tax the overall system. Batch processing is usually the optimal data pipeline when there isn’t an immediate need to analyze a specific dataset (for example, monthly accounting), and it is more associated with the ETL data integration process, which stands for “extract, transform, and load.”

Batch processing jobs form a workflow of sequenced commands, where the output of one command becomes the input of the next command. For example, one command might kick off [data ingestion](https://www.ibm.com/think/topics/data-ingestion), the next command may trigger filtering of specific columns, and the subsequent command may handle aggregation. This series of commands will continue until the data quality is completely transformed and rewritten into a data repository.

Streaming data

Unlike batching processing, [streaming data pipelines](https://www.ibm.com/think/topics/data-pipeline-types)—also known as event-driven architectures—continuously process events generated by various sources, such as sensors or user interactions within an application. Events are processed and analyzed, and then either stored in databases or sent downstream for further analysis.

Streaming data is leveraged when it is required for data to be continuously updated. For example, apps or point-of-sale systems need real-time data to update inventory and sales history of their products; that way, sellers can inform consumers if a product is in stock or not. A single action, such as a product sale, is considered an “event,” and related events, such as adding an item to checkout, are typically grouped together as a “topic” or “stream.” These events are then transported via messaging systems or message brokers, such as the open-source offering, [Apache Kafka](https://www.ibm.com/think/topics/apache-kafka).

Since data events are processed shortly after occurring, streaming processing systems have lower latency than batch systems, but aren’t considered as reliable as batch processing systems as messages can be unintentionally dropped or spend a long time in queue. Message brokers help to address this concern through acknowledgements, where a consumer confirms processing of the message to the broker to remove it from the queue.

Data integration pipelines

Data integration pipelines concentrate on merging data from multiple sources into a single unified view. These pipelines often involve extract, transform and load (ETL) processes that clean, enrich, or otherwise modify raw data before storing it in a centralized repository such as a data warehouse or data lake. Data integration pipelines are essential for handling disparate systems that generate incompatible formats or structures. For example, a [connection](https://dataplatform.cloud.ibm.com/docs/content/wsj/manage-data/conn-amazon-s3.html?context=cpdaas&audience=wdp) can be added to Amazon S3 (Amazon Simple Storage Service)—a service that is offered by Amazon Web Services (AWS) that provides object storage through a web service interface.

Cloud-native data pipelines

A [modern data platform](https://www.ibm.com/topics/data-platform) includes a suite of cloud-first, cloud-native software products that enable the collection, cleansing, transformation and analysis of an organization’s data to help improve decision making. Today’s data pipelines have become increasingly complex and important for data analytics and making data-driven decisions. A modern data platform builds trust in this data by ingesting, storing, processing and transforming it in a way that ensures accurate and timely information, reduces data silos, enables self-service and improves data quality.

Use Cases

As big data continues to grow, [data management](https://www.ibm.com/topics/data-management) becomes an ever-increasing priority. While data pipelines serve various functions, the following are for business applications:

1. Exploratory data analysis: Data scientists use [exploratory data analysis (EDA)](https://www.ibm.com/topics/exploratory-data-analysis) to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods. It helps determine how best to manipulate data sources to get the needed answers, making it easier for data scientists to discover patterns, spot [anomalies](https://www.ibm.com/topics/anomaly-detection), test a hypothesis or check assumptions.
2. Data visualizations: To represent data via common graphics, [data visualizations](https://www.ibm.com/topics/data-visualization) such as charts, plots, infographics, and even animations can be created. These visual displays of information communicate complex data relationships and data-driven insights in a way that is easy to understand.
3. Machine learning: A branch of artificial intelligence (AI) and computer science, [machine learning](https://www.ibm.com/think/topics/machine-learning-pipeline) focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. Through the use of statistical methods, algorithms are trained to make classifications or predictions, uncovering key insights within data mining projects.
4. Data observability: To verify the accuracy and safety of the data being used, [data observability](https://www.ibm.com/think/insights/a-data-observability-model-for-data-engineers) applies a variety of tools for monitoring, tracking and alerting for both expected events and anomalies.

Handling Data Privacy and Integrity

As future IT professionals, you’ll often deal with personal and sensitive data. It’s your job to ensure data is Private – Only accessible to authorized users, and Accurate and untampered – Not altered or corrupted. The Philippines adheres to the Data Protection of its citizens as discussed [here](https://www.globalcompliancenews.com/2024/04/19/https-insightplus-bakermckenzie-com-bm-data-technology-philippines-minimum-requirements-for-security-of-personal-data-issued-by-the-national-privacy-commission_04032024/). While you must be knowledgeable of the basic protection guaranteed to every citizen, you must also be familiar with your company’s data privacy manual, policies, and guidelines so that you’ll surely process data according to the standard procedures.

Some of the common techniques to apply are the following:

1. Encrypt sensitive data (e.g., passwords, student grades)
2. Use access controls (e.g., only HR can access employee records)
3. Apply data validation to prevent errors

You may, however, refer to a more comprehensive list found in this [article](https://careerfoundry.com/en/blog/data-analytics/data-analysis-techniques/).

4. Data Integration and Processing

When an organization uses multiple systems (e.g., POS, CRM, HRIS), data integration connects them so they can “talk” to each other. Processing means organizing or analyzing the collected data for reports or decision-making. It’s like combining data from different departments during a barangay meeting—each has different formats (paper, Excel, verbal), but you need to merge them for a unified barangay report. When a logistics company integrates data from drivers' mobile apps, customer service reports, and warehouse scanners to track deliveries in real time, for example, that’s a good illustration of integration.

Tools for Data Cleaning and Transformation

Before data can be useful, it often needs cleaning and transformation—removing duplicates, fixing inconsistencies, and standardizing formats.

Common Tools:

1. Microsoft Excel/Google Sheets – For small-scale cleaning
2. Python (Pandas library) – For scripting transformations
3. Talend / Apache NiFi – Drag-and-drop tools for large-scale data processing
4. SQL – For cleaning and aggregating data in relational databases

A government agency collects survey data across regions. Some entries say “QC,” others say “Quezon City.” A cleaning tool can standardize these to improve accuracy in reporting.

6. Defining and Managing Data Sources

Before you start any project, you need to know where your data is coming from. These are your data sources—which could be:

1. Internal databases (e.g., student info system)
2. APIs (e.g., weather or maps services)
3. CSV files, spreadsheets
4. Web data or logs

Managing data sources means:

1. Validating data is coming from the right source
2. Keeping records of who owns the data
3. Updating sources when systems change

If your capstone system pulls product data from Shopee’s API, you’ll need to define the API as your data source, understand how often it updates, and what to do if the API changes or fails.

There are more challenges when dealing with data as listed in this [article](https://improvado.io/blog/big-data-analytics-privacy-problems), but equal in number if not more are the [solutions ](https://www.linkedin.com/pulse/8-common-data-challenges-faced-small-medium-sized-impacts-dhindsa)that these challenges continue to trigger. Of course, larger corporations will have better means to mitigate and address these, but what about small and medium enterprises, just like FinMark? In[ OECD SME and Entrepreneurship Papers No. 15](https://www.oecd.org/content/dam/oecd/en/publications/reports/2019/06/data-analytics-in-smes_1535d46b/1de6c6a7-en.pdf), context for SMEs regarding Data Analytics is well discussed. To give you more insights on Project Finer FinMark, read the summary of "Data Analytics in SMEs: Trends and Policies" by Marco Bianchini and Veronika Michalkova:

Chapter 2: SMEs Competing in the Age of Big Data

There is transformative potential of data analytics for SMEs, enabling better decision-making and enhanced competitiveness. Key technological drivers include:

1. Big Data (characterized by Volume, Velocity, Variety, and Veracity)
2. Internet of Things (IoT) – enables real-time data collection
3. Enterprise software like ERP, CRM, and SCM systems
4. Artificial Intelligence (AI) and machine learning technologies
5. Cloud computing – crucial for SMEs lacking large infrastructure
6. Blockchain – increasingly accessible for SMEs via external services

While SMEs rarely possess in-house AI capabilities, cloud-based solutions are democratizing access to advanced analytics.

Chapter 3: Trends in SMEs’ Use of Data Analytics

The chapter uses OECD and Eurostat data to show gradual adoption of data analytics by SMEs, though with clear disparities based on firm size and country.

1. As of 2018, only ~12% of EU SMEs used big data analytics, compared to 33% of large firms.
2. Germany, Malta, and France showed notable increases; Italy and Estonia saw declines.
3. Common data sources:
4. Geolocation (50%)
5. Social media (46%)
6. IoT/smart devices (27%)
7. SMEs increasingly outsource analytics (42% in 2018, up from 36% in 2016).

Software adoption:

1. ERP systems are underused by SMEs, but usage is rising.
2. CRM and SCM adoption remains significantly lower than in large firms.

Cloud Computing:

1. Usage is expanding, particularly for basic services (email, accounting).
2. SMEs are 15% more likely to buy accounting software; large firms more likely to purchase computing power and CRM tools.

Chapter 4: Key Challenges to Adoption

The chapter outlines internal and external barriers to data analytics adoption.

#### Internal Barriers:

1. Managerial ignorance: Many SME leaders lack awareness of data's strategic value.
2. Skill shortage: SMEs struggle to hire data specialists.
3. Cybersecurity fears: Many lack formal security policies.
4. Inadequate data: Many SMEs don’t collect or store data in analyzable forms.

#### External Barriers:

1. Financial constraints: High upfront costs for infrastructure.
2. Access to high-quality data: Limited or expensive third-party data sources.
3. Regulatory complexity: Data protection laws (e.g., GDPR) pose compliance burdens.
4. Lack of SME-tailored solutions: Many tools are designed for larger firms.

Chapter 5: Policy Approaches to Foster Adoption

OECD member governments are taking multi-pronged policy actions:

#### Regulatory Environment:

1. The GDPR provides SME exemptions (e.g., on record-keeping).
2. Governments offer guidelines and support portals for SME compliance.

#### Digital Threats:

1. Campaigns like “Stop. Think. Connect.” in the U.S. raise awareness.
2. Government agencies provide cyber threat alerts and self-assessment tools.

#### Data Localisation:

1. Governments are increasing data localisation rules.
2. These can hinder SMEs’ access to global data flows, calling for a policy balance.

#### Open Data Initiatives:

1. SMEs benefit from government open datasets, especially in tech, health, and legal services.
2. U.S. and EU examples show APIs and satellite data being leveraged by small firms.

Training and Skills:

1. Despite a shortage of ICT specialists, 79% of EU SMEs use internal staff for data tasks.
2. Governments promote training programs to upskill SME staff in data analytics.

Data analytics holds strong potential to improve SME productivity, particularly through 1) Process optimisation, 2) Targeted marketing, and 3)New business models. However, successful uptake requires addressing human capital, infrastructure, and regulatory hurdles simultaneously. The most effective policies offer integrated support across training, regulation, finance, and digital infrastructure.

If you feel you need an example of data flow diagrams, you may refer to Item #3 [here](https://blog.nashtechglobal.com/the-most-common-diagrams-in-it-business-analysis-a-must-know-guide/).

---

# How do you build a platform that works?

Creating a strong, reliable application requires more than just writing code. It demands a clear understanding of how software, network, and data components are designed, integrated, and secured to work together seamlessly. This module explored the foundational elements that every platform builder must consider to deliver applications that are not only functional, but also scalable, maintainable, and secure.

From the software architecture side, you learned how architectural patterns—like layered, microservices, or event-driven designs—guide how systems are structured and maintained. You also considered the importance of scalability and security early in the design process, ensuring that your platform can grow and adapt to real-world usage while protecting sensitive data and processes. Designing and documenting APIs allowed you to define how modules interact, and you explored how integration affects both performance and long-term sustainability of your system.

In the network architecture component, you studied how network topologies, VPNs, firewalls, and intrusion detection systems (IDS) support platform availability and protection. You practiced configuring secure network setups and used tools like iftop, iperf3, and Wireshark to monitor traffic and diagnose performance issues. These concepts taught you how to prepare your application’s infrastructure to handle high traffic, maintain secure connections, and support real-time communications reliably.

On the data architecture side, you focused on designing scalable pipelines and ensuring data flows smoothly from source to output. You applied privacy standards, integrity checks, and schema validations to protect data quality throughout the lifecycle. Using tools like Pandas, Pandera, and ETL frameworks, you practiced transforming raw input into clean, trustworthy datasets that support business insights and user functionality.

﻿

All these elements—software modularity, secure network infrastructure, and reliable data handling—combine to form the foundation of a good application. As you continue to develop your projects, your challenge is to think holistically: every system must be designed to work not just on its own, but as part of a unified, adaptive platform that can respond to scale, protect its users, and deliver consistent results.

---

# Homework: Milestone 1: Project Blueprint (Draft)

By now, your team should have already consulted with your mentor regarding the initial requirements of your task.

This time, submit a draft of your Milestone 1 for initial feedback. Note that this submission will serve as your mentoring session for Milestone 1. Your submission is important—your mentor’s feedback here will help assess your responsiveness to improving your work. If you choose not to submit a draft, you are waiving your opportunity for revisions in Milestone 1. While this is allowed, it’s highly recommended that you take advantage of all opportunities for growth, including feedback from a practitioner with relevant specialization and experience.

To help you refocus on the key expectations of your client, here’s a summary of the main responsibilities per specialization. These are the end-goals of your contract with them, so before the term ends, make sure that these goals are fully achieved:
`OUR FOCUS IS : Network & Cybersecurity`

1. 
2. Network & Cybersecurity: Ensure data security, maintain system stability, and prevent or manage system crashes and data breaches.
3. 

This week, you are tasked to complete the following:

B. Network and Cybersecurity

1. Analyze FinMark’s current network setup and security posture based on the provided context, particularly in relation to:
2. Unauthorized access incidents reported in recent weeks
3. System slowdowns during peak usage hours
4. Potential vulnerabilities that affect platform stability, user trust, and regulatory compliance
5. Key Tasks:
6. Diagram the Current Network Infrastructure
7. Create a diagram illustrating the assumed current network components and data flow.
8. Include elements such as routers, firewalls, servers, DNS, CDN (if applicable), load balancers, and access control systems.
9. Clearly label data flow paths (e.g., external request → internal systems) and note areas where unauthorized access or slow performance may occur.
10. Analyze Bottlenecks and Vulnerabilities
11. Identify specific issues related to:
12. Performance (e.g., latency under load, limited bandwidth)
13. Security (e.g., weak access control, lack of encryption or segmentation)
14. Propose a Redesigned Secure and Scalable Network
15. Develop a revised network architecture that addresses performance and security issues, including:
16. Role-based access control and authentication layers
17. Scalable components (e.g., distributed servers, load balancers, cloud/CDN services)
18. Security features (e.g., updated firewalls, intrusion detection/prevention systems, data encryption)
19. Ensure that legal and policy considerations (e.g., PDPA, GDPR, PCI DSS, or FinMark’s IT Manual) are reflected where relevant.
20. Prepare a Brief Presentation
21. Submit two diagrams: (1) Current network setup and (2) Proposed secure and scalable design
22. Include a short explanation (document or slide deck) that summarizes:
23. Key vulnerabilities and performance issues
24. Design decisions and security improvements

 To fully be guided, follow the instructions below:

1. Access the appropriate templates  [here](https://docs.google.com/document/d/1bcHvH_Xo-6-hzKI0QPPBNUATzujPmdbBOnh9yHeWMB8/copy). Notice that all the specialization tasks are included here. This is for you to also know how your tasks are interconnected.
2. After reading all the tasks, remove those which you won’t be working on.
3. Retain the Intellectual Property Notice page.
4. Note that the templates contain guide questions. The tools for each task have been provided in the previous week. You may explore these tools when appropriate for your diagramming task.
5. Follow the naming convention: MO-IT151 Milestone 1: Draft `<Section>` `<Group Number.>` E.g., MO-IT151 Milestone 1: Draft  A1101 Group #6
6. Submit the link to your copy of this template. This will serve as your submission for this week, where your mentor is expected to make comments on your initial draft.
7. Make sure that the link you will submit is an appropriate one and is accessible to anyone from MMDC can comment. Erroneous link, placeholders, or inaccessible links are considered as non-submission.
