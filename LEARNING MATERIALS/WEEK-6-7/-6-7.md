# Expectations

Yipee! Milestone 1 is finally, truly, completely over! Ready for the next phase of your project in making FinMark finer?

This week marks your official Milestone 2 preparations. You are expected to build upon the first iteration of your Milestone 1 by setting up the core components of your platform project. This step is crucial in preparing for the development and implementation of your platform technology prototype. Your focus should be on translating your plans into functional outputs while staying adaptable to unexpected challenges that may arise during development.

Depending on your specialization, here is what you are expected to accomplish:

1. Software Development:
2. You should begin building a functional application prototype that reflects the key features you outlined in Milestone 1. This includes setting up the user interface, essential backend processes, and integration points. Expect to make real-time adjustments as you test features and address issues related to performance and usability.
3. Networking and Cybersecurity:
4. You are expected to create a working network simulation that shows how devices, servers, and services interact within your platform. Your design should consider secure protocols, access control, and basic fault tolerance. Be prepared to troubleshoot unexpected connectivity or configuration issues as they appear.
5. Data Analytics:
6. You are tasked to develop a functional data pipeline capable of ingesting, processing, and analyzing relevant platform data. This setup should be able to handle real-time or batch data flow and produce insights based on your objectives. You may need to rework your initial design if bottlenecks, data quality issues, or tool limitations arise.

As you use this guide, do the following:

1. Take note of any concept, topic, or terminology that is confusing, unclear, and/or unfamiliar to you. Conduct internet research, post a message and interact with your peers on your decided class platform, or schedule a mentoring session.
2. Manage your time wisely so you can complete the readings and/or activities in Camu before your next synchronous session.

# Expected Output

You are expected to complete the following tasks by the end of the week. Take note of the ones that are required to be submitted on Camu.

1. Complete Milestone 2 preparations based on specialization.
2. Update Project Management Tool.

---

# --- DISCUSSION ---

---

# Core Component Development

When you're designing any software system — whether it's a barangay incident reporting app, a school management system, or an e-commerce platform — one of the first and most important tasks is to identify its core functionality. By now, you should have a clear view of what the core functionality of your platform is, and this week should help you build it in order for it to be tested.

Considering your MS 1 diagrams, can you validate if you can identify your core functionalities? Remember, core functionalities refer to the essential features that your system must provide to serve its main purpose and satisfy users and stakeholders. Before proceeding to the prototyping, below is a step-by-step approach to help you identify and define these core functionalities. This is an important checker before proceeding with prototyping so that you won’t waste your energy for a component that is not yet as needed.

1. Understand the Problem Domain
2. Before writing a single line of code, you need to understand the real-world problem you're trying to solve.
3. What to do: Interview users (e.g., barangay officials, teachers, store owners), observe workflows, and review existing documentation.
4. Example: If you're designing a log-in system for a university portal, understand who uses it (students, faculty), what they log in for (grades, announcements), and what issues they currently face (e.g., frequent password resets or slow login process).
5. Tools & Techniques:
6. Interviews and surveys with users
7. Domain modeling (draw diagrams showing key entities like "User", "System", "Session")
8. Define the System Scope and Boundaries
9. Now that you understand the problem, define what your system will and will not do.
10. What to do: Clearly state the features you'll include, and avoid “scope creep.”
11. Example: For your university log-in system, maybe you’ll include password recovery but exclude student enrollment (that’s a separate module).
12. Techniques:
13. Use Case Diagrams (e.g., “User logs in,” “User resets password”)
14. User Stories: “As a student, I want to log in to check my grades.”
15. Decompose the System into Modules
16. Once you know the scope, break down the system into functional chunks or modules.
17. What to do: Identify the major tasks your system performs and group related functions together.
18. Example: In your log-in system, core modules might include:
19. Authentication Module (verify user credentials)
20. Session Management Module (track active users)
21. Password Reset Module (manage forgotten credentials)
22. Techniques:
23. Functional decomposition
24. Object-oriented modeling (create class diagrams for "User", "Session", etc.)
25. Evaluate System Quality Attributes
26. Functionalities are not enough — your system must also meet quality expectations like speed, security, and scalability.
27. What to do: Identify important non-functional requirements based on your users.
28. Example: For the university login system:
29. Security: Passwords must be hashed.
30. Performance: Login should complete in <2 seconds.
31. Availability: Must be up 99.9% during enrollment season.
32. Techniques:
33. Quality Attribute Scenarios (e.g., “What happens if 1,000 users log in at once?”)
34. Use architectural patterns like MVC or layered design for maintainability
35. Validate System Functionality
36. Before rollout, check if your core functionalities really solve the user’s problem.
37. What to do: Test your modules, run simulations, and let real users try it.
38. Example: Conduct usability testing with 10 students and ask: “Was the login smooth?” “Did the system lock you out too early?” etc.
39. Techniques:
40. Prototyping (e.g., using Figma or an early working build)
41. Unit and integration tests
42. User feedback forms

How do you build a working version of a core feature (e.g., login, basic sync)?

In software development, functional modules are self-contained units that perform specific tasks within a system. They help in:

1. Organizing code for better readability and maintainability
2. Enabling reusability across different parts of the application
3. Facilitating collaboration among developers
4. Isolating errors and bugs for easier debugging

Think of functional modules as the "building blocks" of your platform that ensure your software functions securely and reliably from the ground up.

Key Characteristics of a Well-Designed Module

1. Single Responsibility
2. A module should do one thing only and do it well.
3. Example: A LoginService handles login logic, but not registration or profile updates.
4. Loose Coupling
5. A module should depend as little as possible on other modules.
6. Example: The UserModule does not directly access the database but communicates through a  UserRepository .
7. High Cohesion
8. Related functions and data should be grouped together.
9. Example: All analytics-related functions like trackEvent() and generateReport() go inside the  AnalyticsModule .
10. Well-Defined Interface
11. Modules expose a clear API or set of functions to other parts of the system.

Common Module Structures

#### A. By Feature (Recommended for most student projects)

/src

  └── features/

    └── auth/

    ├── Login.js

    ├── Register.js

    ├── authService.js

#### B. By Layer (Useful for larger systems)

/src

  ├── controllers/

  ├── services/

  ├── repositories/

To better appreciate this, consider this situation. You're part of a student development team tasked to build a Barangay Assistance Platform for a local government unit (LGU) in Bicol. The platform allows barangay officials to log in and report emergencies like flooding, blocked roads, and requests for relief goods.

Because only authorized officials should have access, you need a secure login system. You're using [Node.js](https://nodejs.org/en) with the [Express](https://expressjs.com/) web framework to build the backend.

To keep your code organized, especially as the system grows, you decide to split the login code into three separate files inside an /auth folder. This is called a modular file structure.

Now, understand the login flow below:

#### 1. The Request Comes In

The barangay captain opens the login page, enters their email and password, and clicks Login. This sends a POST request to your server’s /login route.

The request is caught by the first file:

authRoutes.js This file defines the login route. It listens for POST requests to / login and forwards them to the controller.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd-UpukbqUqYUO46lKkItmt0H_xTwhzBRCsLpPtlkLSa8hSyloc_AMFP68jLHQyAaj3gmHl4WY3MVk3WSmUkYSi-B5u74dGNBqVG4YK4E8wsfmFM37g5foL9zd_9meUsGHkk48vGA?key=Id5ySj01pWeEgDxLs3B918MB)

If you want to try this code, copy-paste this:

// authRoutes.js

const express = require('express');

const router = express.Router();

// Import the login controller function

const { loginUser } = require('./authController');

// Route: Handles POST requests to /login

router.post('/login', loginUser);

// Export this router so it can be used in the main app

module.exports = router;

This tells the system: “If someone sends a POST request to /login, handle it using the loginUser function from the controller.”

#### 2. Processing the Login

Now the request goes to the second file:

authController.js This file contains the logic for handling the request — it receives data, calls the service layer, and returns a response.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXehc-WfBNYiMxMleJRYzZxy0Y3K4DL82Z8TTmK4FUI36uzIBFbrsspPS3cuyYbmNbmNhvn1cL-saI1TzD_lCWwFKOAif-EADFluPAJn7rb-H6wbsn02oUPDbT_F8i6VDACUNRoR?key=Id5ySj01pWeEgDxLs3B918MB)

If you want to try this code, copy-paste this:

```javascript
// authController.js

const { verifyLogin } = require('./authService'); // Import login checker from service



// Function to handle login request

const loginUser = async (req, res) => {

  // Destructure email and password from the request body

  const { email, password } = req.body;



  try {

    // Call the service to verify credentials

    const result = await verifyLogin(email, password);



    // If login is successful, return a success response with a token

    res.status(200).json({ message: "Login successful", token: result });

  } catch (err) {

    // If login fails, return an error response with status 401 (Unauthorized)

    res.status(401).json({ error: err.message });

  }

};



// Export the function so it can be used in the route

module.exports = { loginUser };
```

This function does three things:

1. Retrieves the email and password from the user
2. Calls the third file (authService.js) to check if the credentials are correct
3. Returns a success or failure message
4. Verifying Credentials

The third file, authService.js, contains the core login logic. This file performs the business logic — it checks if the credentials are valid.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf0oWL6aY91gNkxWxR_ZYDO3NUr-DGCAj_mEfqaZvMxqnjsLV_UOfzQ-8DCBPXjx4EW7cyoD1dGJ_OIa-m2I_5414qITDutDcbbBupRxWmPWU2EpmHpj-qLvz-ahcto_DG9Bgi6?key=Id5ySj01pWeEgDxLs3B918MB)

Right now, it’s using a dummy user, but in a real version, you'd check:

1. The user’s record in a database (e.g., [MongoDB](https://www.mongodb.com/) or [MySQL](https://www.mysql.com/))
2. If the password matches after hashing (e.g., using [bcrypt](https://nordvpn.com/blog/what-is-bcrypt/#:~:text=Bcrypt%20is%20a%20cryptographic%20hash,cipher%20algorithm%20as%20its%20base.))
3. Then issue a [JWT token](https://auth0.com/docs/secure/tokens/json-web-tokens) for session management

If you want to try this code, copy-paste this:

```javascript
// authService.js



// Dummy user for demonstration purposes

const dummyUser = {

  email: 'student@example.com',

  password: 'secure123'

};



// Function to check if entered credentials match the dummy user

const verifyLogin = async (email, password) => {

  // Basic credential check — in real life, you'd check a database

  if (email === dummyUser.email && password === dummyUser.password) {

    // Simulate returning a JWT token

    return 'fake-jwt-token'; // Replace with real token logic later

  } else {

    // If credentials don't match, throw an error

    throw new Error("Invalid credentials");

  }

};



// Export this function so the controller can use it

module.exports = { verifyLogin };
```

Does it really have to be this structured?

The answer to that is a roaring YES! Say two months later, the LGU asks your team to add the option to reset password, add an admin role access, or include auditing of logins. Because you split the code into three files, you can do the following with ease:

1. Add a resetPassword() function in authController.js without messing with your routing logic
2. Update your authService.js to include role validation
3. Another developer on your team can work on modifying or adding new routes (like /logout or /register) in authRoutes.js, while you can focus on improving the login logic in authService.js or authController.js.

Ultimately, your login system is secure (as it verifies identity before granting access), cleanly structured (as it’s easy for multiple developers to work on), and scalable (as it’s ready for new features like multi-role support).

The barangay captain logs in successfully, sees the dashboard, and reports a blocked road — and your system handles it all efficiently behind the scenes. You will definitely sleep tight at night knowing that you’ve built one good module for a critical function, that is to facilitate ayuda to real people, not any ghost recipients or any Jay Kamote of sorts!

Key Practices in Module Building

In a nutshell, when building modules in software development, especially in backend systems like those using Node.js and Express, following good practices ensures maintainability, scalability, and collaboration. One essential practice is to use a folder structure by feature, where related files (like routes, controllers, and services) for a specific functionality are grouped together, making the codebase more organized and easier to navigate. Moreover, exerting efforts to separate logic between controllers and services — controllers handle HTTP requests and responses, while services manage the business logic, allow each layer to focus on a single responsibility. Developers should also export only the necessary functions to avoid clutter and prevent exposing internal processes that should remain private. Another critical habit is to handle errors gracefully, using structured error-handling mechanisms like try-catch blocks to provide informative, secure responses without crashing the system. Lastly, always add comments and documentation to explain what each part of the code does — this helps not only the original author but also future developers who may maintain or extend the system. Altogether, these practices support cleaner, safer, and more efficient module development.

Consider the checklist below if the codes above display the common practices in the workplace. Can you try judging whether or not each practice is correctly observed?

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeCT4ee6mfX40se60It7Hha68IiXnWH8QC405wsk3GRmjH-APs1-6D_ovHqawYfUHrHvQCbzFtTCwDxlfmyHj0V8P6AUyts6SQYQYIYPl2qIG1If1v5dhRn0lerOm9S7clXJLHfYA?key=Id5ySj01pWeEgDxLs3B918MB)

1. Use folder structure by feature

The code is placed inside a dedicated /auth folder, which groups all files related to authentication (login feature). This follows the "structure by feature" practice rather than lumping everything into one generic controllers or routes folder.

So what? If you add new features later like /reports, /users, or /notifications, you can simply add new folders like /reports/reportController.js, keeping your codebase organized.

2. Separate logic (controller vs service)

The controller file (authController.js) handles how to respond to requests, while the service file (authService.js) handles what to do (business logic, like checking credentials).

So what? This makes it easier to test or change business logic without affecting routing or HTTP handling.

3. Export only necessary functions

Each file only exports what’s needed for the next file to use — no extra clutter.

So what? Keeps your module API clean and avoids accidental exposure of internal functions.

4. Handle errors gracefully

The controller uses a try-catch block to catch login failures and send a proper error response.

So what? Instead of crashing or exposing system internals, the system responds with helpful, safe messages.

5. Add comments and documentation

Inline comments explaining every key line is helpful for other developers of your team should they take over the project.

So what? Future devs (or your teammates) can understand the purpose of each function quickly.

After the discussion above, are your initial answers correct? Check the answers [here](https://www.canva.com/design/DAGkvxgNzh8/V2yu4hBux8X9ce_eRZgNOg/view?utm_content=DAGkvxgNzh8&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h40656ddae0).

Confused with the answers? Try reading this section again or book a consultation with your mentor.

---

# Practice: Module Building

Try doing this 2-part practice for you to build the habit of performing the common key tasks in module building. Make sure you have: A local machine (Windows/Mac/Linux) with Node.js installed, an online IDE access to  [Replit](https://replit.com/) ,   [CodeSandbox](https://codesandbox.io/) , or   [Glitch](https://glitch.com/) , or a cloud platform like AWS Lambda, Heroku, Vercel, or Railway (with Node support).

Part A: Continue with the development of a Barangay Assistance Platform, where users need to create an account before they can log in and submit emergency reports. For this task, your role is to build a Register Module using the same modular structure you've practiced with the login feature.

 Your Task:

 Create the following files inside a new  /register  folder:

1. registerRoutes.js  – for defining the  /register  route
2. registerController.js  – for handling the incoming request and calling the service
3. registerService.js  – for validating and storing the user data (in-memory)

 Requirements:

1. Accept user input:  email  and  password
2. Validate the inputs:
3. Email must be in proper format (e.g., contains  @  and a domain)
4. Password must be at least 6 characters
5. Store the user in a simple array (no database yet, just memory)
6. Return a success response on valid input, and error messages for invalid input

Part B: You’ve already built a working login or register feature in a single script file during your initial practice. While it may function correctly, keeping all logic (routing, request handling, validation, storage) in one file makes your code hard to maintain and scale — especially in real-world projects where many developers work together.

In this follow-up task, you’ll refactor that flat script into a clean, modular structure based on what you've learned in Practice Task 1.

Your Task:

1. Take an existing script (e.g., a full login or register logic written in  app.js  or a single file)
2. Break it into three separate files inside a feature-based folder (e.g.,  /login  or  /register  ):
3. featureRoutes.js
4. featureController.js
5. featureService.js
6. Add basic error handling using  try-catch  blocks in the controller.
7. Implement logging to track success and failure events (console logging is fine for now).

Did it work? Way to go, SD geek!

Otherwise, what appears to be a challenge for this practice task? Review this section if there's an error appearing in your code. Compare your work with this  [ ZIP](https://drive.google.com/file/d/18CzwssvAzzZ31Pez3gHiwV6pEz8oV9ds/view?usp=drive_link)  file. Where could your error be coming from?

It’s best to inform your mentor about your progress at this point. They’ll surely appreciate your progress. Set a consultation session so that as early as now, you’ll know how to figure out where the error might be coming from. Request for a live demonstration so that you may also observe how the thought process works!

---

# Testing and Debugging Techniques

Software testing is the process of evaluating a system or its components to check whether it meets the required outcomes. As IT students who aim to build real-world platforms — like an inventory or order system for food delivery startups — testing ensures that your work is usable, secure, and reliable.

There are different types of testing techniques which you can use depending on your skill, available technologies, and needs of your client. Whether you're debugging a broken form or validating a working discount flow, testing is your safety net. Learn about manual and automated testing in this section. Your key takeaway is to understand that combining manual testing to understand how users interact and automated testing to scale and secure your system long-term is a strategic practice software developers must know how to do.

If you think you already know these, feel free to skip this part and move to the next page.

A. Manual Testing

Manual testing is done by a person without using automated tools. It’s suitable during the early stages of development, exploratory testing, and UI testing.

    1. Functional Testing - This type tests if each function of the platform works as expected.

1. Use Case: You test if a user can successfully register with a valid email and password.
2. Scenario: On your microenterprise platform, you manually fill out the registration form and check if the form submits, redirects, and stores the data correctly.

Checklist:

1. Does clicking “Submit” trigger the right response?
2. Is the user redirected correctly after login?
3. Are error messages shown for wrong passwords?

   Common in : Login modules, inventory creation, or POS systems
4. Usability Testing - This type checks how user-friendly the platform is.
5. Use Case: You ask a classmate to simulate placing an order. If they struggle to find the “Place Order” button, that’s a UX problem.
6. Scenario: You build a dashboard for sari-sari store owners. During testing, someone clicks “Sales History” but doesn’t understand what “Filter by SKU” means.

 Common in : Checkout pages, dashboards, forms

3. Exploratory Testing - This test is used when you explore the system without a set script and you’re just trying to find unexpected behavior.
4. Use Case: You randomly try incorrect inputs, like submitting an empty cart or entering emojis in a product name field.
5. Scenario: You test what happens if you click "Submit" multiple times while the page is still loading.

 Common in : Form submissions, AJAX buttons, quick releases

4. Regression Testing - You manually re-test existing features to make sure new changes didn’t break them.
5. Use Case: You just added a discount feature. Now you test if placing an order without a discount still works.

 Common in : Any platform with frequent updates

B. Automated Testing

Automated testing uses scripts or tools to test your application automatically. It's great for repetitive tests, large datasets, or continuous integration pipelines.

1. Unit Testing - This type tests individual functions or components in isolation.
2. Use Case: Test a function that calculates product discounts.
3. Scenario: Your calculateTotal() function should return 950 for a ₱1000 item with 5% discount.

Frameworks:

1. JavaScript/Node.js: Jest, Mocha
2. Python: Pytest, Unittest
3. Java: JUnit

 Common in : Pricing logic, discount calculation, stock level checks

2. Integration Testing - This tests how multiple modules work together.
3. Use Case: Test how your registerUser() service integrates with the user database and the confirmation email module.
4. Scenario: You simulate a registration and verify if the user is stored and an email is queued.
5. Tools:
6. Node.js: Supertest (API), Postman (manual but supports automation too)
7. Python: requests + pytest

 Common in : Registration workflows, order placements, reporting systems

3. End-to-End (E2E) Testing simulates real user actions from start to finish.
4. Use Case: Automate a test where a user logs in, adds an item to cart, and checks out.
5. Scenario: This ensures the entire platform works, from frontend input to backend database updates.

Frameworks:

1. Cypress (modern, fast, JavaScript-based)
2. Selenium (cross-language, supports Java, Python, JS, etc.)
3. Playwright (fast and flexible for full browser testing)

 Common in : Web-based platforms with multiple user journeys

4. Performance Testing checks how the system performs under load — speed, response time, stability.
5. Use Case: Test how your system handles 100 people clicking "Buy" at once.
6. Scenario: You simulate heavy traffic on the microenterprise site during a flash sale.
7. Tools:
8. [Apache JMeter](https://jmeter.apache.org/)
9. [Locust (Python)](https://locust.io/)
10. [Artillery (JavaScript)](https://www.artillery.io/)

 Common in : E-commerce, inventory management platforms, sale events

---

# Debugging and Error Handling

Testing lays the groundwork for effective debugging by helping you identify what should happen versus what actually happens. When debugging, you build on your tests — retracing each step like solving a mystery — from the user's experience back to the system’s expected behavior to find where things break. It is like reverse engineering a mystery, where you start from what the user sees, trace it back to what the system should be doing, and catch it where it goes wrong.

When things break (and they will), use these tools to trace problems:

| Tool                                                                              | Usage                                           | Example                                                   |
| --------------------------------------------------------------------------------- | ----------------------------------------------- | --------------------------------------------------------- |
| [Browser Dev Tools (Chrome)](https://developer.chrome.com/docs/devtools)             | Inspect HTML, test JS, monitor network requests | Find why an AJAX call to/registerfails                    |
| [VS Code Debugger](https://code.visualstudio.com/docs/editor/Debugging)              | Step through code, inspect variables            | Pause insidevalidateAndStoreUser()to see incorrect values |
| [Postman](https://www.postman.com/)                                                  | Test APIs, simulate requests                    | Send POST/loginwith wrong data and inspect JSON response  |
| [Console.log / console.error](https://www.w3schools.com/jsref/met_console_error.asp) | Basic logging for tracing events                | Log each step in login to identify where it fails         |

For these weeks, focus on runtime errors. These are errors that occur while the program is running, not while it is being compiled or initially loaded. These errors can crash your app, produce incorrect results, or silently fail — making them hard to catch unless properly tested and debugged.

There are common types of runtime errors, and each type has a different strategy to be employed. Check out the list below and the examples where these may be observed:

1. Null or Undefined Values - Trying to access a property or method of something that doesn’t exist.

Example:

console.log(user.name); // Throws error if user is undefined

In a POS platform, the cart is emptied after checkout, but the confirmation page still tries to show cart.items[0].name. This leads to Cannot read property 'name' of undefined.

What will you do?

Use fallback/default values or check for null:

if (cart?.items?.length > 0) {

   console.log(cart.items[0].name);

} else {

  console.log("No items found");

}

Doing this prevents your app from crashing during the final step of checkout. The user sees a friendly message instead of a broken screen.

2. Invalid Data Format - An operation receives a value with the wrong structure or data type (e.g., text instead of a number).

Example:

const quantity = parseInt("two"); // NaN

const total = quantity  100;  // NaN

On your microenterprise inventory system, a cashier types "three" instead of 3 in the quantity field. The system doesn’t validate the input and ends up calculating totals as NaN.

What will you do?

1. Add input validation before processing.
2. Display a warning if the input format is incorrect.

if (isNaN(quantity)) {

  return res.status(400).json({ error: "Quantity must be a number" });

}

This keeps transaction totals accurate and avoids misreporting income or inventory depletion.

3. Unhandled Exceptions in Promises or Async Functions - A failed async operation (like a fetch or database call) that isn’t wrapped in a try/catch or .catch() block.

Example:

const user = await getUserById(id); // If this fails, the app may crash

The POS system checks the database for product info, but the product was deleted. The unhandled promise rejection causes the app to stop responding.

What will you do?

Wrap async calls in try/catch blocks or use .catch().

try {

  const user = await getUserById(id);

} catch (err) {

  console.error("Database error:", err.message);

  res.status(500).send("Something went wrong");

}

This prevents crashes from unexpected backend issues and improves reliability under poor network conditions.

4. Timeouts and Delays - Operations that take too long to respond or hang indefinitely (e.g., API calls or database queries).

Example:

fetch('/checkout').then(...); // Takes >10 seconds to respond, user clicks again

A user clicks “Checkout” and nothing happens for 15 seconds because the server is slow. The user clicks again, causing duplicate orders.

What will you do?

1. Set timeouts on requests
2. Disable buttons while waiting
3. Show loading states

setTimeout(() => {

  alert("The server is taking too long to respond.");

}, 5000);

This improves user experience by giving feedback and avoids duplicate transactions.

5. Type Errors (Calling Something That's Not a Function or Method) - Calling a method on a variable that doesn’t support it.

Example:

let total = 100;

total.push(10); // TypeError: total.push is not a function

In your report generator, the summary is expected to be an array, but a logic bug accidentally sets it to a string.

What will you do?

Check types or use validation libraries (like Joi, Yup, or custom checks).

if (!Array.isArray(summary)) {

  throw new Error("Summary must be an array");

}

This prevents hidden logic bugs that break reporting features or analytics dashboards.

In a nutshell, here are the five runtime errors you may encounter as you build your app:

| Error Type           | Common Cause                          | Handling Strategy                      |
| -------------------- | ------------------------------------- | -------------------------------------- |
| Null/Undefined       | Missing data or uninitialized vars    | Use fallback values, optional chaining |
| Invalid Format       | Wrong data type (e.g., string vs int) | Input validation, type checks          |
| Unhandled Exceptions | Failed API/database call              | try/catch blocks,.catch()              |
| Timeouts             | Network slowness, long operations     | Set timeouts, loading states           |
| Type Errors          | Misuse of object types                | Type checks, use validation libraries  |

---

# Checkout: Module Building, Testing, and Debugging

This whole chapter is not just about finding errors in codes. That should be an elementary task for you at this point. What you have to realize at this point, however, is that when you start building your platform, you are not just literally building your modules. You are also testing, debugging, building, then testing again, debugging again. Remember, a platform becomes trustworthy not because it never fails, but because it knows how to recover when it does! Module building, testing, and debugging are NOT stages — they are habits. They work together to ensure that your platform is not just functional, but resilient, maintainable, and meaningful to the businesses and communities that depend on it.

So, go back to the scenario above and see the question differently:

You were hired to build a POS platform (A point of sale (POS) system is a combination of hardware and software that helps businesses process transactions and manage sales. It's the place where a customer pays for their purchase) for a small apparel store. The store owner reports, however, that clicking “Checkout” sometimes does nothing. This obviously results in loss of sales as customers get frustrated when even after deciding they’ll buy something, the platform would not allow them to.

What does a failed “Checkout” button mean? Check all that applies.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfBkVLO1PFT5mePgMMpbTCDEFoOmD2y-i-E2cLDxuO1VhQDmphmzU5crtlXFb8vJUSPzhOBO3i4_V9QKLUENGHX5ekyEv4dS-dirbAymUwj2wcsDNp-CLHN_UczzBrlUBZ_JWbZGA?key=Id5ySj01pWeEgDxLs3B918MB)

![unchecked]()The event isn’t bound to the button (onClick not working)

![unchecked]()The button is disabled after one click and doesn’t reset

![unchecked]()The API call silently fails due to bad inputs or timeouts

![unchecked]()There’s a logic error in calculateTotal()

![unchecked]()A race condition or concurrency issue in how cart items are processed

Check the correct answers [here](https://www.canva.com/design/DAGkxVb5HOM/g_19-1vSQqSYApbPlv1jTQ/watch?utm_content=DAGkxVb5HOM&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h35cc0b2e4c).

As the developer, you’re expected to test across the full flow — from the button click to the API response — because the problem could exist at multiple layers:

1. In the frontend interaction, where the “Checkout” button may not trigger the event due to a missing listener or a disabled state.
2. In the business logic, where something like calculateTotal() may be returning incorrect values or not handling invalid inputs.
3. In the API layer, where the server may be timing out, returning errors, or failing silently.
4. Or in the integration across layers, where each component appears to work independently but fails when combined (e.g., a valid form sends bad data because of misaligned expectations between frontend and backend).

This approach reflects the essential cycle of building, testing, and debugging:

1. You build each component cleanly and modularly.
2. You test each one (and their connections) thoroughly.
3. And when things still go wrong — as they often do in real-world apps — you debug across layers to find the root cause.

A developer doesn’t just fix what’s broken — they investigate patterns, validate assumptions, and improve the resilience of the system with each error they resolve.

As part of your project, it’s useful to come up with a checklist of the things and means to debug. Think of it as a future repository for your own projects or notes for your junior developers. In this POS Checkout Button example, here’s a sample [checklist](https://docs.google.com/document/d/1mMjoPUk8yuZV-OvUX2Pd1zL_tPHhABnFUsbX6PX1R3k/view).

---

# Practice: Debugging and Testing

Simulate this checkout scenario with intentionally faulty inputs. Then, implement error handling so that your app doesn’t crash, and the user still receives a meaningful response even if the server fails.

You built a Point of Sale (POS) app for a small apparel store. When a customer checks out, your frontend app sends order details to the backend API (/api/checkout). The API is expected to return a receipt, which is then shown on a receipt screen.

However, customers report that:

1. Sometimes clicking Checkout does nothing.
2. Other times, the screen crashes.
3. In some cases, no receipt appears — just a blank screen.

Can you identify what’s wrong with the code:

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcPscIXCJ8hTxBi0ZU2-zUVIwVRdS5m_yUmVSTLPyjBJF-HswhqR3OBXepzuVEazbT8otqFUB6C2PmINBSgm4hA1e58Mg62YrFq-H47UGuch37ook625HksNur7i_hUnErn2fQbOQ?key=Id5ySj01pWeEgDxLs3B918MB)

Which of the following makes the code buggy?

1. ![unchecked]()Null input – order.customer is null; might cause issues on the backend
2. ![unchecked]()Invalid format – quantity is a string (“two”) instead of a number
3. ![unchecked]()No error handling – If the API fails or times out, the app crashes
4. ![unchecked]()No fallback UI – If checkout fails, the user sees nothing
5. ![checked]()All of the above

Try fixing the code above. Refer to the flowchart for you to have an idea on what the ideal logical flow is:

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdvHeOUWlowEsOtWiseD37qZA_QuUqy0rKN3ufs4_twU4Ua6jTvqvrE02jsi6xl594M-bONYPQfttlA7ktZMqmuKsuXQbnKTrvKXyfrVquqLppXjtehSR1zGjAjOj6ptssIsLQQmA?key=Id5ySj01pWeEgDxLs3B918MB)

Were you able to successfully debug the code?

If you feel this is too easy, watch this [video](https://www.youtube.com/watch?v=bTT-KGIzpsw) and share your thoughts on tools and techniques in the Conversations tab.

---

# Traffic Optimization and Performance Management

Network traffic (see inbound and outbound diagram  [ here](https://sc1.checkpoint.com/documents/IaaS/WebAdminGuides/EN/CP_Azure_VMSS_GWLB/Content/Topics-Azure-VMSS-GWLB/Network-Diagram.htm)  ) refers to the flow of data packets across a network as devices communicate with each other and with servers over the internet or local systems. It includes everything from website requests, API calls, streaming services, file transfers, to background syncs from various applications.

Analyzing and optimizing network traffic involves monitoring these data flows to identify bottlenecks, reduce latency, and ensure that critical services receive the bandwidth they need.

For example, in an e-commerce platform, delays in checkout requests or dropped packets during payment confirmation can directly result in lost sales. Through traffic analysis, network administrators can uncover patterns of congestion or misuse, and apply optimization techniques such as Quality of Service (QoS), traffic shaping, or protocol prioritization to enhance overall performance and reliability.

Think of a network like a busy highway, like EDSA, during rush hour.

A network administrator is like a traffic manager watching all the cars (data packets) moving along the roads (network channels).

Through traffic analysis, the manager notices:

1. Some roads are always jammed at certain times (congestion).
2. Delivery trucks (important data) are stuck behind slower vehicles (less critical traffic).
3. Some cars are looping around unnecessarily (misuse or inefficient routing).

To fix this, the manager applies solutions:

1. Quality of Service (QoS) is like assigning express lanes for ambulances and delivery trucks so they move faster.
2. Traffic shaping is like setting speed limits or staggered entry times to control the flow of vehicles.
3. Protocol prioritization is like giving VIP passes to emergency vehicles so they always go first.

The result? Smoother, faster, and more reliable traffic — just like a well-optimized network.

Watch this short video before proceeding to the rest of this section:

<iframe class="ql-video ql-indent-2 ql-align-center" frameborder="0" allowfullscreen="" src="https://www.youtube.com/embed/sNry6SYez7Y?si=0mDP6QfSPQ9-1Tbr" height="382" width="700"></iframe>

 Analyzing and Optimizing Network Traffic

As data constantly flows between users, servers, and services, maintaining the efficiency, reliability, and security of this traffic becomes critical. This is where network traffic analysis and optimization come in. Read through this chapter to explore how to observe, interpret, and manage traffic flows using industry-standard tools. You will also learn how to make informed decisions that improve user experience, system reliability, and overall network health.

Analyzing network traffic involves monitoring the flow of data packets across the network to identify patterns, detect bottlenecks, and troubleshoot issues. It helps administrators answer key questions like: Who is using the most bandwidth? Are there any unauthorized activities? Why is the system running slowly?

 Optimizing network traffic takes this a step further by applying configurations and rules that improve performance. This may involve shaping traffic, prioritizing critical services, or limiting non-essential data transfers—especially during high-demand periods.

The following are key concepts under analyzing and optimizing network traffic:

1. Network Throughput and Latency

Throughput is the amount of data successfully transferred over a network in a given period, typically measured in Mbps or Gbps. Latency, on the other hand, is the time it takes for a packet to travel from source to destination, measured in milliseconds (ms). These terms are often interchangeably used, but know that throughput, latency, (and bandwidth even) differ from each other:

Several related terms --  [ throughput, bandwidth and latency](https://www.techtarget.com/searchnetworking/feature/Network-bandwidth-vs-throughput-Whats-the-difference)  -- are sometimes mistakenly interchanged. Network bandwidth refers to the capacity of the network for data to be moved at one time. Throughput expresses the amount of data that can be processed. Latency is the time it takes for data to travel from one point to another.  Network throughput and latency together reflect a network's performance  (Burke, 2025).

Think of throughput as how much water flows through a pipe per second. Latency is how long it takes one drop of water to reach the faucet after opening it.

Consider this scenario:

A Taguig-based online jewelry shop notices that image-heavy product pages load slowly. On investigation, it turns out that though their internet speed (throughput) is decent, the delay (latency) when fetching images from a US-based server is high.

Basic Check with [CLI](https://aws.amazon.com/what-is/cli/#:~:text=A%20command%20line%20interface%20(CLI)%20is%20a%20text%2Dbased,operating%20system%20and%20the%20user.):

Test latency

 ping www.google.com

Test throughput (via speedtest CLI)

 speedtest-cli

2. Bandwidth Utilization

 Bandwidth is the maximum data capacity of your connection (e.g., 100 Mbps), while utilization refers to how much of it is being used at a given time. As stated above, it is often interchanged with throughput, but by now you should know their key differences:

 ![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeZr86hPw7_3-VLvK-Aqpap4yMYMEVBpglsWknddqt6eWK4i_zM0CpaRp0CgMPdepPGIRdbKtrTbgU3_fB89OHCKUn0VTeQFUY4X2krUrvYfhAmw3L1y5xTDEFRtqD96Nlz0msJ?key=Id5ySj01pWeEgDxLs3B918MB)

For example, If a school has a 20 Mbps connection and during lunch hours everyone is streaming YouTube, the network might be using 19.9 Mbps — nearly maxed out, leaving little room for essential operations. Another is when you buy from a sari-sari store that offers e-loading and online payments. Between 5–6 PM, employees also stream videos, leading to slow response when customers try to scan QR codes. Bandwidth is being used up by non-essential services. This is why when you are too busy with school work, your colleague may ask you, “Do you still have bandwidth for this?” asking about your maximum utilized capacity.

How to Monitor:

Real-time bandwidth usage per interface (Linux)

iftop

3. Packet Loss and Retransmission

Packet loss occurs when [data packets](https://www.cloudflare.com/learning/network-layer/what-is-a-packet/) (small units of information, like little data "packages," that are transmitted over networks like the internet) fail to reach their destination. Retransmission is when the sender has to resend lost packets, increasing network load (the amount of traffic or data transmitted across a network at any given time) and reducing performance. Like sending text messages with missing words — the receiver has to ask you to repeat until the message is complete.

Consider this scenario:

A Bacolod-based online retailer uses mobile data as their internet connection. During thunderstorms, the LTE signal fluctuates, causing dropped packets during order submission.

Detection and Analysis:

Check for packet loss

ping -c 10 8.8.8.8

Example output shows % packet loss

4. Traffic Shaping and Quality of Service (QoS)

Network traffic refers to the flow of data packets across a network, whether they’re web requests, file transfers, video streams, or API calls. Managing this traffic is essential to prevent congestion and ensure critical services function properly. Traffic shaping controls the flow of traffic to ensure bandwidth is reserved for important data. QoS prioritizes certain types of traffic (e.g., VoIP, checkout API calls) over others. It’s like giving an express lane to emergency vehicles on EDSA — the most critical services get to pass first.

For example, a Cebu-based flower delivery platform prioritizes real-time tracking updates and checkout confirmations over uploading high-res images to the product catalog. QoS ensures smoother transactions even during high traffic.

Simple Linux Traffic Shaping (tc command):

Set limit to 512kbps for a non-essential port (e.g., port 8080)

sudo tc qdisc add dev eth0 root handle 1: htb default 12

sudo tc class add dev eth0 parent 1: classid 1:12 htb rate 512kbps

Real-time vs Bulk Traffic Behavior

Real-time traffic (e.g., video calls, VoIP, online payments) requires low latency and minimal jitter. Bulk traffic (e.g., downloads, backups, image uploads) can tolerate delays but consumes a lot of bandwidth.

Illustration:

1. Real-time: Zoom call freezing = bad experience
2. Bulk: Uploading 200 product images = can wait a bit longer

An e-commerce company in Sipalay schedules product image uploads to 2 AM, when network load is low, while prioritizing transaction traffic during the day.

How to simulate this:

Simulate bulk traffic (download test)

wget --limit-rate=500k https://speed.hetzner.de/100MB.bin

Simulate real-time traffic (video call or voice traffic test)

iperf3 -c server-address -u -b 512k -t 30

Tools and Techniques for Analyzing and Optimizing Network Traffic

When managing network traffic, visibility is crucial. Network administrators use a variety of tools to monitor, analyze, and control traffic to ensure the network operates efficiently and securely. Below are commonly used tools that you can explore and apply in simulation-based activities.

#### 1. [Wireshark](https://www.wireshark.org/): Packet Capture and Protocol Analysis

When investigating network issues or analyzing detailed data flows, it’s important to have a tool that can inspect traffic at the packet level.

Wireshark is a free and widely used network protocol analyzer that captures live packet data from a network interface and allows users to examine each packet in detail. It provides deep visibility into protocols such as [TCP](https://www.cloudflare.com/learning/ddos/glossary/tcp-ip/), [UDP](https://www.cloudflare.com/learning/ddos/glossary/user-datagram-protocol-udp/), [HTTP](https://www.cloudflare.com/learning/ddos/glossary/hypertext-transfer-protocol-http/#:~:text=The%20Hypertext%20Transfer%20Protocol%20(HTTP,A%20typical%20HTTP%20request%20contains:), and [DNS](https://www.cloudflare.com/learning/dns/what-is-a-dns-server/#:~:text=The%20Domain%20Name%20System%20(DNS,dedicated%20to%20answering%20DNS%20queries.), making it a valuable tool for troubleshooting, security analysis, and learning how data travels across a network.

Functionality:

1. Capture and inspect packets in real time
2. Analyze protocols (TCP, UDP, HTTP, DNS, etc.)
3. Filter traffic based on IP addresses, ports, and packet types

Use Case: In an e-commerce platform, some customers report that logging in or checking out takes unusually long, or sometimes fails entirely. To investigate, the developer uses Wireshark to capture network packets during the login and checkout processes. By analyzing the packet flow, they identify whether the delay is caused by slow server responses, API timeouts, or DNS resolution problems. This helps isolate the root cause and allows the team to fix the specific issue—whether it’s at the application level or the network layer.

How It’s Used in Simulation

In your simulation, you can replicate a user attempting to log in or make a transaction on a test website. Use Wireshark to capture packets during that interaction. Apply filters (e.g., http, ip.addr == your_server_ip) to focus only on relevant traffic. Then, analyze packet timing and error codes to understand how each step in the communication process performs. This practice will help you trace performance issues back to either the app server, DNS resolver, or the client itself.

Challenge for Simulation: Try this [Cisco Packet Tracer](https://www.netacad.com/cisco-packet-tracer) and start with any one of its Beginner level self-paced courses. You may also watch this [beginner’s guide ](https://www.youtube.com/watch?v=5nS_2zA-kDw)from Wireshark.

#### 2. [NetFlow](https://www.ibm.com/think/topics/netflow) or [sFlow](https://www.cisco.com/c/en/us/td/docs/iosxr/cisco8000/netflow/configuration/b-netflow-configuration-ios-xr-8000/monitor-traffic-using-sflow.html#:~:text=Recording%20of%20Packet%20Flows%20in%20sFlow,-The%20packet%20in&text=Here's%20how%20sFlow%20handles%20the,traffic%20patterns%2C%20and%20other%20metrics.): Traffic Monitoring in Enterprise Settings

NetFlow (Cisco) and sFlow (multi-vendor) are technologies used for collecting IP traffic information. These tools are typically used in switches or routers to analyze bandwidth usage across the entire network. They are technologies used to summarize data about network flows (who’s talking to whom, how much data, over what protocol, etc.). These summaries are exported from devices like routers or switches to a collector — a tool or server that stores, analyzes, and displays the data.

Functionality:

1. Provide flow-level visibility without capturing entire packets
2. Identify which IPs, applications, or users are consuming the most bandwidth
3. Detect top talkers and heavy flows in a network

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdMCDpBSykXU25S7UOrLJUDCwQGTeq4KrSCnBWXXOBTakor8mcrV7SZmVf5j-FOll8BhdNJyLr5M8vLF7zWOUnTrH30CkzkFJNzFV-aJCiiWB5aYK7rFYElTzaJa1SrOcn4mJzU?key=Id5ySj01pWeEgDxLs3B918MB)

A distribution warehouse experiences slow performance in its POS (Point of Sale) terminals. To investigate, the IT team enables NetFlow monitoring on the core switch to track how bandwidth is being used between the POS terminals and the central server. The NetFlow data reveals that a large internal backup process was running during business hours, using up most of the bandwidth. With this insight, the team reschedules the backup task to off-peak hours, restoring normal system performance.

How It’s Used in Simulation

In your simulation, you can set up a router or a virtualized lab environment (e.g., using GNS3 or MikroTik CHR) to export NetFlow or sFlow data to a flow collector. Then, use a tool like Ntopng, SolarWinds (trial), or FlowViewer to visualize traffic patterns. You’ll be able to observe how bandwidth is distributed between endpoints, identify top talkers, and simulate how one service (like a backup or bulk file transfer) can impact overall network performance.

#### [3. Ntopng](https://www.ntop.org/products/traffic-analysis/ntop/): Real-Time Traffic Analytics

To better understand network behavior in real time, administrators often rely on tools that offer both traffic visibility and intuitive visual reporting.

Ntopng is a web-based traffic analysis and monitoring tool that provides visual dashboards and detailed statistics on live network traffic. It captures and analyzes data such as top talkers, protocol distribution, interface usage, and host behavior, helping users quickly identify traffic patterns, bottlenecks, or unusual activity on the network.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcfVTmJy2ABU-ebbxz0oulk59jQuRT43RsIOQofgYu6EhkQtuRTCZI5r_7qkJfW9EBvdXWEUKSrBTgZjALLBW2wE8vASTu4rfD4zCV_pkjeUeCf0DNpDZ01Pli-Haglfp94EGsjoQ?key=Id5ySj01pWeEgDxLs3B918MB)

[Image Source](https://i0.wp.com/www.ntop.org/wp-content/uploads/2020/03/Dashboard-1.png?ssl=1)

Functionality:

1. Real-time traffic dashboards showing protocols, top talkers, and geolocation
2. Tracks flows, interfaces, bandwidth usage, and device behavior
3. Works as a collector for NetFlow/sFlow data

Use Case

A local café provides public Wi-Fi but notices that the internet slows down during peak hours. The network administrator installs Ntopng to monitor which devices and websites are using the most bandwidth. With this information, the café can create policies to prioritize essential services (like POS systems and online orders) and limit access to high-bandwidth activities (like video streaming or large downloads), keeping the network stable for business operations.

How It’s Used in Simulation

In your simulation, you can install Ntopng on a virtual machine to act as your traffic monitor. Configure your router or switch (real or virtual) to export sFlow or NetFlow data to Ntopng. From the Ntopng dashboard, you’ll be able to visualize bandwidth usage by IP, service, or protocol. If you're just getting started or using minimal hardware, you can also try FlowViewer, a simpler tool that provides visual summaries of network flows without complex setup.

#### [4. Linux Traffic Control](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/linux-traffic-control_configuring-and-managing-networking#inspecting-qdisc-of-a-network-interface-using-the-tc-utility_linux-traffic-control) (tc) and [QoS](https://www.fortinet.com/resources/cyberglossary/qos-quality-of-service#:~:text=Quality%20of%20service%20(QoS)%20is,prioritizing%20specific%20high%2Dperformance%20applications.) Settings

In Linux-based systems, managing network traffic at the interface level is essential for shaping performance and enforcing prioritization rules.

tc, short for traffic control, is a command-line utility in Linux used to configure and manage Quality of Service (QoS), traffic shaping, and rate-limiting policies. It allows administrators to control how traffic is queued, delayed, or prioritized on a given network interface, ensuring that critical services receive the bandwidth they need while limiting non-essential or bulk traffic.

Functionality:

1. Limit bandwidth for specific ports or services
2. Prioritize critical services (e.g., HTTP over [FTP](https://learn.microsoft.com/en-us/connectors/ftp/))
3. Apply queuing disciplines to manage congestion

A small online business notices that during periods of high customer activity, their checkout system slows down or fails. After investigating, the developer finds that non-essential traffic—such as users downloading high-resolution product images—is using up too much bandwidth. To fix this, they use the Linux tc (traffic control) tool to limit bandwidth on port 8080 (used for image downloads) and prioritize port 443 (used for secure checkout transactions), ensuring that purchases go through even during peak load.

How It’s Used in Simulation

In your simulation, you can create a test environment using virtual machines running multiple services (e.g., a web server on port 80, a file upload service on port 21, and checkout on port 443). Using the tc command, you can apply bandwidth limits to less important services and prioritize traffic to the checkout system. After applying these rules, you can use tools like iperf, curl, or ab (Apache Benchmark) to test how well the system performs under simulated high traffic.

Example:

sudo tc qdisc add dev eth0 root handle 1: htb default 12

sudo tc class add dev eth0 parent 1: classid 1:12 htb rate 1mbit

#### 5. [SNMP-Based Monitoring](https://www.datadoghq.com/knowledge-center/network-monitoring/snmp-monitoring/)

Another important aspect of traffic analysis and network optimization is monitoring the performance and status of network devices in real time.

SNMP, or Simple Network Management Protocol, is a standard application-layer protocol used to collect, organize, and exchange information about devices on a network. It enables administrators to query and manage routers, switches, servers, printers, and other networked equipment by retrieving key data such as bandwidth usage, CPU load, or device uptime. SNMP plays a crucial role in ensuring that network infrastructure remains healthy, responsive, and secure.

If you feel you need more explanation, read this [article](https://www.pubnub.com/learn/glossary/snmp/).

Functionality:

1. Monitor CPU, memory, bandwidth usage, and interface status
2. Send alerts for failures or [threshold breaches](https://docs.oracle.com/cd/E19089-01/n1.sysmgr11/819-2666/fqnri/index.html)
3. Integrates with tools like [Cacti](https://www.cacti.net/), [Zabbix](https://www.zabbix.com/), or [PRTG](https://www.paessler.com/prtg)

An online printing service needs to keep its network and devices running smoothly. It uses SNMP to monitor key resources such as printer queues (to detect slowdowns), router bandwidth (to avoid congestion), and server health (to catch issues early). When a problem occurs—like a switch port going offline or bandwidth usage going over 80%—an alert is automatically triggered so the admin can respond immediately.

How It’s Used in Simulation

In your simulation, you can configure SNMP on virtual machines, routers, or switches. Then, use monitoring tools like SNMPwalk (for CLI-based polling) or Zabbix (for dashboard-based monitoring) to retrieve device statistics. You can also set thresholds, such as alerting when CPU usage is too high or when an interface goes down, to simulate real-time incident detection.

What Can Go Wrong Without Analyzing Network Traffic

Understanding how data flows through a network is essential to maintaining system performance, security, and reliability. Without proper network traffic analysis, several setbacks may arise, especially when managing high-demand environments like e-commerce platforms. Below are six common issues that highlight the importance of monitoring and analyzing network traffic:

1. No awareness of what traffic is passing through your network: Without analyzing active traffic, you cannot determine which applications or users are consuming your bandwidth. This may result in bandwidth hogging from non-essential services or expose your system to threats like DDoS attacks from unknown sources.
2. No visibility into network elements or their reliability: When network devices and interfaces are not actively monitored, it becomes difficult to know which parts of your network are functioning properly. Failure to identify outdated or malfunctioning hardware can affect access speed, stability, and service availability.
3. Limited insight into the users or applications causing slowness: Slowness in a network can come from many sources—bulk file uploads, video streaming, or poorly optimized applications. Without traffic insights, you're left guessing which users or protocols are responsible, making it harder to address performance issues without unnecessary spending on bandwidth upgrades.
4. Inability to shape or prioritize bandwidth usage: Without tools for traffic shaping or Quality of Service (QoS), all data is treated equally. This can lead to non-critical traffic (such as media downloads or software updates) consuming the bandwidth needed for time-sensitive operations like customer checkouts or payment processing.
5. Difficulty planning for future network requirements: As usage grows, so do performance demands. Without data on current utilization and traffic patterns, estimating future capacity becomes a guessing game. This often leads to either overspending on unnecessary upgrades or failing to meet user expectations during peak load.
6. Challenges in detecting abnormal network behavior: Establishing a traffic baseline is essential to recognizing unusual spikes, drops, or patterns. Without this reference, abnormal activities—such as intrusion attempts, malware communications, or sudden surges—can go unnoticed, leading to delayed response and potential downtime.

Failing to analyze network traffic leaves an organization blind to both external threats and internal inefficiencies. It can result in undetected security breaches, poor network performance, compliance issues, slow response to incidents, and a general lack of control over the network environment. Proactive traffic analysis is essential for maintaining a secure, efficient, and compliant IT infrastructure.

---

# Practice: Optimizing Networks

You are managing the network for a local e-commerce fashion brand preparing for a one-day flash sale. Thousands of users are expected to browse, add items to their carts, and proceed to checkout simultaneously. During peak activity, some customers experience delays, while others are redirected incorrectly. Your task is to investigate these issues and optimize the network to ensure critical transactions are prioritized.

Objective:

Your goal is to analyze network traffic during a simulated flash sale. Determine whether checkout-related packets are being delayed, dropped, or misrouted. Explore whether non-critical traffic—such as product image downloads or background syncs—can be deprioritized using traffic shaping or QoS rules to ensure that the transaction experience remains smooth and responsive.

Tools Needed:

1. Wireshark – Packet capture and protocol inspection
2. iftop or nload – Real-time bandwidth monitoring
3. iperf3 – Traffic generation and bandwidth stress testing
4. tc (Linux) or pfSense WebGUI – Apply QoS rules and traffic shaping policies

Instructions:

1. Simulate a high-load event using iperf3 or custom traffic-generation scripts.
2. Use Wireshark to capture live traffic during the simulation.
3. Apply filters (e.g., tcp.port == 443) to isolate checkout traffic.
4. Monitor live bandwidth usage using iftop or nload.
5. Identify signs of congestion or bottlenecks:
6. High retransmissions
7. Packet loss
8. Latency spikes
9. Apply traffic shaping using tc or the pfSense dashboard to:
10. Prioritize checkout traffic (e.g., HTTPS on port 443)
11. Limit bandwidth for non-critical services (e.g., image downloads on port 8080)
12. Re-test to confirm performance improvements under load.

Deliverables:

1. Screenshot(s) showing monitored traffic or bandwidth patterns before and after QoS rules were applied
2. A brief written summary of issues detected (e.g., API call delays, bandwidth saturation)
3. Key issues identified during analysis
4. Specific actions taken to optimize traffic
5. Outcomes observed after optimization
6. Configuration script or screenshots of the traffic shaping setup

PRO TIP!

1. Plan Your Simulation Carefully

Define peak load: Decide what “thousands of users” means in terms of data and concurrent connections (e.g., simulate 500–1000 connections using iperf3).

Create realistic test traffic: Use a mix of traffic types—simulate web browsing (HTTP/HTTPS), image downloads, and background syncs alongside critical checkout activity.

2. Organize Your Monitoring Workflow

Start with a baseline: Run a test without any traffic shaping to capture “normal” congestion and bottlenecks.

Use filters in Wireshark:

Filter on checkout-related traffic (e.g., `tcp.port == 443` for HTTPS or your specific API port).

Also inspect traffic on ports commonly used for non-critical data (e.g., images or static assets).

Live bandwidth tools: Monitor with iftop or nload to see which hosts or ports are using the most bandwidth in real time.

3. Identify and Document Issues

Look for trouble signs:

High retransmissions, packet loss, or latency in the Wireshark capture

Spikes or drops in bandwidth on critical ports during checkout bursts

Non-critical traffic (like images) consuming disproportionate bandwidth during peak times

Take screenshots of relevant charts, lists, or protocol conversations for your deliverables.

4. Apply and Test Traffic Shaping

Prioritize critical traffic:

With `tc` or pfSense, set high priority for checkout (HTTPS/API) traffic.

Assign lower priority and bandwidth caps to image downloads or background updates.

Be precise with rules: Clearly specify which ports or traffic types are high vs. low priority.

Save your configuration scripts or take screenshots for submission.

5. Re-Test and Validate

Run the simulation again with QoS rules active.

Monitor for improvements:

Check if checkout delays and misroutes are reduced.

Confirm that non-critical traffic is limited without blocking essential functions.

Compare before-and-after screenshots to clearly show impact.

6. Summarize Clearly

In your write-up:

List the issues observed during the first test (e.g., "Checkout packets delayed due to image download spikes").

Explain the QoS/traffic shaping approach you applied.

Highlight the outcome (“After prioritizing port 443, checkout latency dropped by X% and no further misroutes observed”).

Attach configuration scripts or dashboard screenshots.

---

# Load Balancing Techniques

Load balancing is the method of distributing incoming traffic across multiple servers or resources so that no single server gets overloaded. It helps ensure that users get fast, reliable access to a website or application—even when many users are online at the same time.

In a network setup, load balancing acts like a traffic officer that directs requests to the right server. When done properly, it keeps all servers working efficiently, reduces the chance of crashes, and improves overall system performance.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXds1suIqrCCx7jTqd-vFjailVfXyKbRaHmLb2jI5AHgpgWUIIcVOaxve1_8X1ZoBBZ-0e6EryL5rDDwfaJNVsrJQaoXYpHlceVSp7qeYaHncB9zC_-IxixkmwkQGoPP7flWAkR2?key=Id5ySj01pWeEgDxLs3B918MB)

[Image Source](https://images.app.goo.gl/sVfTriV8gJy6KyoE8)

Why do you need load balancing?

Prevents overload: If all requests go to one server, it may slow down or stop working. Load balancing spreads the load.

Improves performance: By sharing the work, each server can respond faster to user requests.

Increases availability: If one server fails, a load balancer can redirect traffic to the remaining working servers.

Supports scalability: As demand grows, you can add more servers behind the load balancer without changing how users access the site.

Imagine an online fashion store holding a weekend sale. Thousands of customers visit at the same time. Instead of one server handling everything, the site uses a load balancer to send some users to Server A, some to Server B, and so on. If Server B stops responding, the load balancer automatically sends all new requests to Server A and Server C—keeping the store online.

#### When Is Load Balancing Needed?

1. A single server cannot handle all incoming requests efficiently.
2. Downtime on one server should not cause service interruption.
3. Services need to scale horizontally to meet user demand.

[What are the types of load balancing?](https://aws.amazon.com/what-is/load-balancing/#:~:text=Load%20balancing%20is%20the%20method,a%20fast%20and%20reliable%20manner.)

There are three main categories depending on what the load balancer checks in the client request to redirect the traffic.

1. Application load balancing
2. Complex modern applications have several server farms with multiple servers dedicated to a single application function. Application load balancers look at the request content, such as HTTP headers or SSL session IDs, to redirect traffic.
3. For example, an ecommerce application has a product directory, shopping cart, and checkout functions. The application load balancer sends requests for browsing products to servers that contain images and videos but do not need to maintain open connections. By comparison, it sends shopping cart requests to servers that can maintain many client connections and save cart data for a long time.
4. Network load balancing
5. Network load balancers examine IP addresses and other network information to redirect traffic optimally. They track the source of the application traffic and can assign a static IP address to several servers.
6. Network load balancers use the static and dynamic load balancing algorithms described earlier to balance server load.
7. Global server load balancing
8. Global server load balancing occurs across several geographically distributed servers.
9. For example, companies can have servers in multiple data centers, in different countries, and in third-party cloud providers around the globe. In this case, local load balancers manage the application load within a region or zone. They attempt to redirect traffic to a server destination that is geographically closer to the client. They might redirect traffic to servers outside the client’s geographic zone only in case of server failure.
10. DNS load balancing
11. In DNS load balancing, you configure your domain to route network requests across a pool of resources on your domain. A domain can correspond to a website, a mail system, a print server, or another service that is made accessible through the internet.
12. DNS load balancing is helpful for maintaining application availability and balancing network traffic across a globally distributed pool of resources.

Tools and Technologies

Load balancers are one of two types: hardware load balancer and software load balancer.

1. Hardware load balancers - A hardware-based load balancer is a hardware appliance that can securely process and redirect gigabytes of traffic to hundreds of different servers. You can store it in your data centers and use virtualization to create multiple digital or virtual load balancers that you can centrally manage.
2. Software load balancers - Software-based load balancers are applications that perform all load balancing functions. You can install them on any server or access them as a fully managed third-party service.

Comparison of hardware balancers to software load balancers

Hardware load balancers require an initial investment, configuration, and ongoing maintenance. You might also not use them to full capacity, especially if you purchase one only to handle peak-time traffic spikes. If traffic volume increases suddenly beyond its current capacity, this will affect users until you can purchase and set up another load balancer.

In contrast, software-based load balancers are much more flexible. They can scale up or down easily and are more compatible with modern cloud computing environments. They also cost less to set up, manage, and use over time.

1. HAProxy – High-performance TCP/HTTP load balancer with support for multiple balancing algorithms.
2. Nginx – Web server that also functions as a powerful Layer 7 load balancer.
3. pfSense – Offers built-in load balancing and failover configuration.
4. Cloud-based Load Balancers – AWS Elastic Load Balancer, Azure Load Balancer (not used in this lab, but important to know).

Consider this scenario:

During a flash sale, the e-commerce platform’s backend experiences a spike in checkout requests. While some servers are idle, others are overloaded and crash. Customers receive timeout errors, resulting in failed purchases and abandoned carts.

Objective

Configure a load balancer to distribute traffic efficiently across multiple backend servers. Ensure the system continues to respond even when one server becomes unavailable.

Setup

1. Create 2 or more backend servers (e.g., Apache or Nginx instances on VMs or Docker containers)
2. Deploy [HAProxy](https://www.google.com/aclk?sa=l&ai=DChcSEwjlzvP25eOMAxVxo2YCHXTcApkYABABGgJzbQ&co=1&gclid=CjwKCAjwk43ABhBIEiwAvvMEB84-TzXH5th03wIHbH6TQOa2vhsp-6Do31ZIwJgpo4x8cYwowgX06BoCjS4QAvD_BwE&ei=oW4DaI_ALf2O4-EPjYvT4QU&sig=AOD64_1v1XoWG0oSKEQaKKyOCEIHq8tMYg&q&sqi=2&adurl&ved=2ahUKEwjPlO725eOMAxV9xzgGHY3FNFwQ0Qx6BAgKEAE) or [Nginx](https://nginx.org/) as the load balancer
3. Generate synthetic traffic using curl, ab, or iperf3
4. Monitor behavior as load increases

Instructions

1. Install and configure HAProxy or Nginx to balance traffic across at least two servers.
2. Apply a basic round-robin configuration and verify that requests are distributed.
3. Simulate one backend going offline and observe how the load balancer responds.
4. Implement a health check and retry mechanism.
5. Explore a secondary strategy (least connections or weighted) and compare outcomes.

Sample Configuration (HAProxy)

bash

frontend http_front

   bind :80

   default_backend web_back

backend web_back

   balance roundrobin

   server web1 192.168.1.101:80 check

   server web2 192.168.1.102:80 check

﻿Deliverables

1. Screenshot of load balancer logs or dashboard
2. Summary of behavior under different load scenarios
3. Explanation of chosen load balancing strategy and its impact
4. Optional: Failure recovery test report (e.g., backend server crash + automatic rerouting)

---

# Network Security

In connected environments—where digital platforms serve customers, store sensitive information, and manage critical operations—network security plays a central role in protecting systems from both disruption and exploitation. Can you still think of processes that are in isolation and are equally successful with those connected ones? Whether you're building a platform for financial services, e-commerce, or public infrastructure, safeguarding your network is no longer optional—it's fundamental.

Network security involves more than just firewalls and passwords. It includes actively monitoring how your network behaves under stress, responding to threats in real time, and fixing problems without exposing the system to further risk. As networks grow in complexity and face increasing pressure from legitimate traffic and malicious attacks, professionals must be ready to secure their systems proactively and intelligently.

If you feel you need a simpler introduction to Network Security, see this video:

<iframe class="ql-video ql-align-center" frameborder="0" allowfullscreen="" src="https://www.youtube.com/embed/rG02r5y2Fdo?si=UOdle3QHLc2MXNqX" height="315" width="560"></iframe>

Handling traffic surges and mitigating attacks

As you start your network security attempts, you must first learn how to detect, respond to, and recover from traffic surges and targeted attacks that could overwhelm your system.

A traffic surge refers to a sudden increase in data packets or connection requests entering a network. This can be the result of a legitimate spike in user activity—such as payday transactions in a fintech platform—or the result of a coordinated malicious effort like a Distributed Denial of Service (DDoS) attack. Traffic surges must be analyzed in real time to distinguish between expected user behavior and suspicious overload attempts.

Rate limiting is one strategy used to manage high traffic volumes. It controls the number of requests a user or IP address can make over a period of time. This prevents a single user or script from overwhelming system resources. In financial platforms, rate limiting is commonly applied to login attempts or API endpoints to prevent brute-force or bot attacks.

DDoS mitigation involves techniques such as blacklisting IPs, geo-blocking, or rerouting traffic through content delivery networks (CDNs) to absorb the load. Financial applications, especially digital wallets and banking APIs, are prime targets of DDoS attacks aimed at service disruption or masking deeper intrusions.

Intrusion Detection and Prevention Systems (IDPS) provide another layer of protection by identifying patterns in the traffic that resemble known attack vectors. These systems either alert admins (detection) or take automated action (prevention), such as dropping packets or blocking source IPs.

A fintech app that processes loan approvals and mobile wallet top-ups observes a massive spike in traffic at 2 AM—outside its usual peak period. A quick inspection reveals thousands of repetitive requests to a single API endpoint. This could signal a scripted DDoS attack or credential-stuffing bot. By applying rate limiting and reviewing intrusion detection logs, the admin identifies multiple malicious sources and blocks them before service is disrupted.

Sample Commands

Simulate traffic spike on an endpoint

bash

ab -n 5000 -c 100 https://api.finbank.com/transfer

 Apply basic rate limiting with iptables

 bash

 sudo iptables -A INPUT -p tcp --dport 443 -m limit --limit 20/second --limit-burst 50 -j ACCEPT

Snort rule to detect excessive HTTP requests

snort

alert tcp any any -> any 443 (msg:"High volume HTTPS access"; threshold:type threshold, track by_src, count 30, seconds 10; sid:2000001;)

Secure Troubleshooting Techniques

Now that you know how to detect security issues, try to focus on secure troubleshooting techniques—the skills needed to investigate and fix issues without compromising the integrity or privacy of your network.

Secure troubleshooting ensures that while diagnosing or resolving network issues, the confidentiality and integrity of system data and access are preserved. In sensitive platforms such as online banking systems or fintech APIs, improperly handled diagnostics can expose user data, internal configurations, or access credentials.

Secure remote access is a foundational practice in which administrators use encrypted protocols (such as SSH or VPN) to access systems. This prevents credentials and commands from being exposed over the network. In fintech environments, remote access should also be audited to maintain accountability.

Logging and auditing are essential for traceability. Every troubleshooting action—from restarting services to applying patches—should be recorded. This not only supports transparency but also provides evidence during security reviews.

Packet analysis with filters allows administrators to diagnose network behavior while minimizing the risk of capturing or exposing sensitive information. Instead of capturing all traffic, analysts should filter by IP, port, or protocol to narrow down the issue.

The principle of least privilege emphasizes that administrators should only use the minimum level of access required for the task. For example, a technician reviewing failed transaction logs should not have the ability to change routing tables.

Practical Use Case

A digital payment gateway experiences intermittent timeouts during peak hours. To investigate, an administrator securely accesses internal servers using SSH, ensuring credentials are not exposed. Instead of capturing all network data, they use tcpdump with filters to target only port 443 (HTTPS) traffic, minimizing the risk of collecting sensitive customer information. Every action is logged, allowing the audit team to later review the steps taken. The root cause—a misconfigured load balancer—is identified and fixed without exposing any user data or introducing new security risks.

Free Tools and Platforms

1. [SSH](https://www.techtarget.com/searchsecurity/definition/Secure-Shell#:~:text=SSH%20(Secure%20Shell%20or%20Secure,and%20management%20of%20networked%20systems.)) (OpenSSH) – Secure remote terminal access
2. [tcpdump](https://www.tcpdump.org/) – Lightweight command-line packet capture
3. [Auditd](https://www.insentragroup.com/nz/insights/geek-speak/modern-workplace/mastering-auditd-in-rhel-ensuring-security-through-auditing/) – System activity auditing tool on Linux
4. [Wireshark](https://www.wireshark.org/) (with filters) – GUI-based packet inspection
5. [Zabbix](https://www.zabbix.com/) /[ Nagios Core](https://www.nagios.org/?utm_feeditemid=%2Cutm_device%3Dc%2Cutm_term%3Dnagios%20core%2Cutm_source%3Dgoogle%2Cutm_medium%3Dppc%2Cutm_campaign%3D%7Butmcampaign%7D%2Chsa_cam%3D22203148771%2Chsa_grp%3D176125445404%2Chsa_mt%3De%2Chsa_src%3Dg%2Chsa_ad%3D731788509561%2Chsa_acc%3D%7B6435916521%7D%2Chsa_net%3Dadwords%2Chsa_kw%3Dnagios%20core%2Chsa_tgt%3Dkwd-348961956421&utm_term=nagios%20core&utm_campaign&utm_source=adwords&utm_medium=ppc&hsa_acc=6435916521&hsa_cam=22203148771&hsa_grp=176125445404&hsa_ad=731788509561&hsa_src=g&hsa_tgt=kwd-348961956421&hsa_kw=nagios%20core&hsa_mt=e&hsa_net=adwords&hsa_ver=3&gad_source=1&gclid=CjwKCAjwk43ABhBIEiwAvvMEBxgkhZrNTbw-KCa1EslfQZh4yMkceJbIu3z9kXg9UqstpOQit9nGjhoCqbIQAvD_BwE) – Monitoring and alerting platforms

Sample Commands

Securely access a remote system

bash

ssh admin@10.10.20.30

Capture only HTTPS traffic to a specific IP using tcpdump

bash

sudo tcpdump -i eth0 port 443 and host 10.10.20.30 -w secure-capture.pcap

View audit logs related to iptables (firewall configuration changes)

bash

ausearch -x /usr/sbin/iptables

Wireshark display filter to focus on secure transaction traffic

ini

ip.addr == 10.10.20.30 and tcp.port == 443

---

# Core Pipeline Development

Modern platforms—from financial apps to e-commerce systems—depend on reliable data pipelines to move and process information efficiently. Core pipeline development involves setting up the steps that collect, clean, transform, and deliver data so that it’s ready for use in reporting, analytics, or automation. In this section, you’ll learn how to build functional data pipelines and apply validation and testing techniques to ensure that data flows are accurate, secure, and scalable across your system.

Building functional data pipelines

A data pipeline is a set of processes that automate the movement and transformation of data from one system to another. It often includes stages like data ingestion, cleaning, transformation, storage, and delivery. In analytics environments—such as fintech platforms—pipelines move data from user activity logs, transaction systems, or external APIs into databases or dashboards used for real-time monitoring and reporting.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdyWaGBD7z3JHWT7ssDu89SJjy1Mjbb342JFeJEk5VuREGUnNO6gsF6UOZtnXhf3AbAtiIy-bq8yQVoGyRdlFd-IsO7SkgdaBCHNpSQIMhzXQbMUiu_RKIy3fbJ8tNiaJxCOjyeNQ?key=Id5ySj01pWeEgDxLs3B918MB)

[Image Source](https://www.astera.com/type/blog/etl-pipeline-vs-data-pipeline/)

[ETL (Extract, Transform, Load)](https://aws.amazon.com/what-is/etl/) is a common model, where raw data is first extracted from sources, cleaned and formatted (transformed), and then loaded into a data warehouse or analytics engine. Modern data pipelines may also follow ELT or streaming-based patterns.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfDjjFQVEaduxXRbl95BEw3U1NeE7n2v_lcoFb1RMb6K7Ibzj8jDAy3ESl72ZRhV5VWim9tnMwb6wMsNqLBO-yPO6db29MbKOa07lyDyA-yzVRP3-fAeeypA-lV1-7cOn5MFxhJSw?key=Id5ySj01pWeEgDxLs3B918MB)

[Image Source](https://www.astera.com/type/blog/etl-pipeline-vs-data-pipeline/)

Pipelines should be modular, reusable, and scheduled or triggered based on events or time. They must also handle failure gracefully, log errors clearly, and allow for easy scalability.

Choosing Between a Data Pipeline and an ETL Pipeline: What Should You Use?

When you're building a data solution—whether for a fintech app, an e-commerce platform, or a logistics system—one of the first decisions you'll need to make is whether to use a data pipeline or an ETL (Extract, Transform, Load) pipeline. These two concepts are often used interchangeably, but they are not the same. The best choice depends on your platform’s specific requirements for speed, data structure, and processing flow.

Understanding the Difference

An ETL pipeline is a type of data pipeline focused specifically on extracting data from sources, transforming it to fit a required structure, and loading it into a data warehouse or database. These are ideal for scheduled, batch processing, such as uploading historical sales records every night.

A data pipeline, on the other hand, is a broader term. It may include ETL steps, but it can also handle real-time streaming, data movement without transformation, or even orchestration between services. This makes data pipelines more flexible for modern, cloud-based systems where data flows continuously.

How to Choose Based on Your Use Case

When deciding between the two, you should look at the kind of data you're handling and how fast your platform needs to process it.

1. If you're processing daily or weekly summaries, such as uploading monthly billing data to your reports dashboard, an ETL pipeline is usually enough.
2. If you're working on real-time systems, like monitoring mobile wallet transactions or fraud detection for a loan platform, a data pipeline that can stream and respond immediately is a better fit.

Consider this scenario:

A fintech startup in Quezon City processes GCash transaction logs every 10 minutes to detect suspicious spending patterns. To get near-instant alerts, they use a data pipeline that streams logs and flags anomalies in real time.

Flexibility and Data Structure

ETL pipelines excel in handling unstructured or semi-structured data, because the transformation step can clean, enrich, and convert the data into a format that's easy to analyze. This is useful in scenarios like tax reporting, where raw financial records need to be cleaned and categorized before submission. Data pipelines, especially when built for streaming, are better suited for uniform data that doesn't need heavy transformation. For example, a logistics platform tracking vehicle GPS data might use a data pipeline that forwards timestamped coordinates with minimal processing.

Complexity and Maintenance

ETL pipelines typically involve more upfront setup, since transformation logic must be carefully defined. However, modern tools like Talend, Apache NiFi, and Pentaho make this easier with drag-and-drop interfaces and automation features. Data pipelines can be simpler to set up for basic streaming, but more complex if you add real-time transformation, error handling, and orchestration. For example, using Apache Kafka + Spark requires a deeper understanding of distributed systems.

Tools and Ecosystem

1. ETL pipelines work well with traditional business intelligence tools and data warehouses like MySQL, PostgreSQL, or Snowflake. They’re ideal for platforms that manage structured reports or dashboards, such as those used in corporate finance or HR systems
2. Data pipelines are more often paired with modern, scalable technologies like Apache Kafka, Google Cloud Dataflow, or AWS Kinesis. They are the preferred choice for cloud-native or big data projects.

For example, a Manila-based ride-hailing service uses a Kafka-based data pipeline to stream location, payment, and driver activity in real time—feeding live dashboards used by operations managers.

Take note:

1. ETL pipelines specialize in extracting, cleaning, and loading structured data—perfect for batch processes and reporting systems.
2. Data pipelines offer more flexibility and support real-time data flow, orchestration, and scalability—great for event-driven systems and analytics.

Despite their differences, these pipelines can and often do work together in real-world platforms. For instance:

1. You can use ETL to prepare historical customer data from accounting software.
2. You can also use a data pipeline to stream real-time transaction data into your fraud detection service.

The goal isn’t to pick one over the other—it’s to use them strategically together where they make the most impact.

Check out the free Tools and Platforms you may want to explore for this course:

[Apache Airflow](https://airflow.apache.org/)

Why it’s useful: It teaches students how to orchestrate data workflows using Directed Acyclic Graphs (DAGs). This is valuable when building multi-step pipelines (e.g., extract → validate → transform → load).

Best for: Task scheduling, pipeline automation, retries, dependencies

Use case: A fintech app running a nightly summary job that pulls daily transactions, cleans them, and updates a report dashboard.

[Apache NiFi](https://nifi.apache.org/)

Why it’s useful: Great for students who are more visual learners. NiFi allows drag-and-drop design of data flows, with built-in processors for filtering, transforming, and routing data.

Best for: Easy integration, quick testing of data flows, low-code exploration

Use case: A logistics firm pulling sensor data from delivery trucks, transforming formats, and forwarding them to storage.

[Python](https://www.python.org/) + [Pandas](https://pandas.pydata.org/)

 Why it’s useful: This is the foundational toolkit for data analytics. Most students already know Python, and Pandas enables them to clean, manipulate, and explore datasets programmatically.

Best for: Custom scripts, fast prototyping, manual data processing

Use case: A startup cleansing and reshaping a customer behavior dataset for exploratory analysis or machine learning.

[Talend Open Studio](https://www.talend.com/products/talend-open-studio/)

Why it’s useful: Talend bridges the gap between enterprise ETL and student-friendly design. It helps students understand structured ETL logic through a GUI-based approach.

Best for: Building batch-oriented pipelines, transforming structured data

Use case: A government system aggregating monthly reports from various regional offices into a centralized database.

[Kafka](https://kafka.apache.org/) + [Spark](https://spark.apache.org/)

Why it’s useful: These are advanced-level tools for students interested in real-time systems and distributed processing. Ideal for projects involving large-scale, fast-moving data.

Best for: Stream processing, event-driven pipelines, real-time analytics

Use case: A payment gateway using Kafka to stream transaction logs and Spark to detect fraudulent patterns as they happen.

Sample Pipeline with Python and Pandas

python

import pandas as pd

# Extract

df = pd.read_csv('transactions.csv')

# Transform

df['timestamp'] = pd.to_datetime(df['timestamp'])

df = df[df['amount'] > 0] # Filter invalid transactions

# Load

df.to_parquet('cleaned_transactions.parquet')

Data validation and testing techniques

As data flows through a pipeline, errors and inconsistencies can quietly accumulate—leading to faulty reports, broken dashboards, or incorrect business decisions. That’s why data validation and testing are critical components of building reliable data systems.

Data validation ensures that incoming data meets the required structure, type, and quality standards before it’s processed or stored. This includes checking for missing values, invalid formats, duplicates, or out-of-range entries. On the other hand, data testing involves verifying whether each step of your pipeline performs as expected—whether it’s a transformation function, a cleaning script, or the final output.

In the succeeding paragraphs, you’ll learn how to build automated checks, enforce data schemas, and apply testing techniques that help you catch issues early. These practices not only improve the trustworthiness of your data but also make your pipelines more scalable, maintainable, and production-ready.

Key Concepts

1. Data validation ensures the data meets expected formats, rules, and business logic before being used for decision-making. It may involve checking for missing fields, incorrect types, duplicate entries, or logical inconsistencies.
2. Testing in data pipelines includes both unit testing (individual components like transformation functions) and end-to-end testing (entire pipeline output).
3. Validation can occur at multiple stages—before ingestion, after transformation, or before storage.
4. Schema enforcement, assertions, and data quality checks help detect errors early and avoid faulty reports or models.

Practical Use Case

A financial data pipeline processes loan applications daily. One day, an unexpected system change causes the interest rate column to load as strings instead of floats. Without validation, this leads to broken analytics and inaccurate reporting. A validation layer could have caught this before it reached the dashboard.

Tools for Data Validation and Testing – What You Can Use and Why

When you're working with data pipelines, it's not enough to just clean and move data—you also need to validate it and test your pipeline steps to make sure your output is correct, complete, and trustworthy. These tools will help you do that more efficiently and professionally.

[Great Expectations](https://greatexpectations.io/)

If you want to set up automated rules that check your data before it gets stored or used, Great Expectations is a powerful tool. You can define what "good data" looks like—like setting rules that a column must be numeric, contain no nulls, or fall within a certain range.

1. Why use it? It gives you a structured, readable way to validate data quality.
2. Try it when: You're working with batch datasets, like customer data or survey results, and want to flag bad rows before loading them into your warehouse.

[Pandera](https://pandera.readthedocs.io/en/stable/)

If you're already using Python and Pandas (which you probably are), Pandera makes it easy to add schema validation directly into your code. It works like a contract that your DataFrame must follow.

1. Why use it? You can validate column types, ranges, and constraints as part of your pipeline script.
2. Try it when: You're building a transformation pipeline and want to make sure nothing breaks if the input data changes.

[pytest](https://pytest.org/) + Python

Every time you write a function that transforms or calculates something, you should test it. With pytest, you can write unit tests for your custom logic and integration tests to ensure each stage of your pipeline works as expected.

1. Why use it? It helps you catch errors in your logic early and automate the testing of your pipeline components.
2. Try it when: You're writing functions like currency conversion, timestamp formatting, or outlier filtering.

[Airflow Test Operators](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/operators.html)

If you’re using Apache Airflow to orchestrate your pipeline, you can also test whether each task runs correctly or produces the expected output using test operators or assertions.

1. Why use it? It gives you control over what happens if a task fails, and lets you automate rollback or alerting.
2. Try it when: You're running a multi-step job and want to make sure that each stage (like extract → clean → load) completes successfully before the next one begins.

Suppose you have a CSV file called `transactions.csv` with the following columns:

1. `transaction_id` (should be a string)
2. `amount` (should be a positive float)
3. `timestamp` (should be a valid date and time)

You want to ensure that all data loaded from this file meets these requirements before you use it in your analytics pipeline. This approach ensures your transaction data is clean, valid, and ready for analysis—protecting you from data quality errors that could break your analytics or lead to wrong results.

You can use [Pandera](https://pandera.readthedocs.io/) to automatically validate your data as soon as it is loaded.

#### How it works:

1. Step 1: Define the expected structure using `pa.DataFrameSchema`.
2. `"transaction_id"` must be a string.
3. `"amount"` must be a float and greater than zero.
4. `"timestamp"` must be a valid date-time value.
5. Step 2: Load your data with Pandas.
6. Step 3: Use `schema.validate(df)` to check that all rows in your DataFrame follow the schema.
7. If the data has missing, wrongly-typed, or invalid values (e.g., negative amounts), Pandera will  raise an error , helping you catch issues early.

By integrating Pandera or any other acessible tool into your data workflow, you can automate the process of checking data quality and instantly catch any inconsistencies or errors before they affect downstream analytics. These tools not only streamline data validation but also increase confidence in the integrity of your analyses, ensuring that business decisions are based on accurate and reliable information. Robust data validation is both accessible and efficient for data teams of any size through the help of these tools.

---

# Practice

You’ve now covered the essential concepts and tools needed to build a functional and trustworthy data pipeline. You’ve learned the difference between a data pipeline and an ETL pipeline, and how each can be applied depending on whether you're working with real-time or batch data.

You’ve also explored practical tools for data ingestion and transformation, such as Python + Pandas for scripting, and Apache NiFi or Airflow for managing flow and scheduling. For data validation, you’ve been introduced to Pandera and Great Expectations, both of which allow you to define rules that ensure your data meets expected formats and quality standards.

On top of this, you’ve practiced how to log events, handle errors properly, and enforce data schemas—skills that are crucial when you're processing sensitive data like customer transactions in a fintech system.

As you prepare for the simulation, remember that you're not just cleaning data—you’re designing a repeatable, testable, and trustworthy pipeline. You’ll be applying real-world practices to catch problems early, prevent bad data from slipping through, and ensure your system remains reliable and scalable.

Use this task as a chance to bring everything together and demonstrate how you can turn raw, messy input into clean, validated output—just like you would in a production environment. Refer to this [Dataset](https://docs.google.com/spreadsheets/d/1SaRZ99qMo0FmlUcyoqJULShSmam5Q5RpUHLm3hUlWtE/edit?gid=726421516#gid=726421516). You may choose between Set A and Set B. You may even do both!

Scenario

You are working for a digital payment provider. Your task is to create a pipeline that ingests customer transaction logs, transforms them into a clean dataset, and performs schema validation before loading into a storage format (CSV, Parquet, or database).

Deliverables

1. A Python-based ETL pipeline (or using Airflow/NiFi)
2. Validation rules defined using Pandera or Great Expectations
3. Log output showing success and error handling
4. A brief summary of what types of errors were caught and how

Remember:

| Step                              | Tool                             | Purpose                   |
| --------------------------------- | -------------------------------- | ------------------------- |
| Ingest & Transform                | Python + Pandas                  | Easy to script and debug  |
| Schedule & Orchestrate (optional) | Airflow / NiFi                   | Helps manage larger flows |
| Validate                          | Pandera or Great Expectations    | Enforces schema and rules |
| Log Errors                        | Built-in Python logging or print | Shows what failed and why |
| Output                            | Save to CSV/Parquet or SQLite    | Simulates database load   |

---

# Data Challenges

As you continue to build data-driven systems, you’ll quickly realize that not all data behaves the way it should. Real-world datasets are rarely perfect—they may contain missing values, corrupted records, inconsistent formats, or arrive at unpredictable volumes and speeds. These issues can break pipelines, distort analysis, and lead to poor business decisions if not handled properly.

This reading on Data Challenges will help you recognize and respond to these problems early. You’ll explore techniques for detecting and fixing missing or corrupt data, and learn how to design pipelines that remain scalable and resilient, even under pressure. By addressing these challenges head-on, you’ll build systems that are not only functional—but also dependable, maintainable, and ready for production.

Ready, Data Doctor?

Handling missing or corrupt data

Missing or corrupt data refers to entries in a dataset that are incomplete, incorrectly formatted, or entirely unusable. This is common in environments where data is sourced from multiple systems—such as mobile apps, APIs, or external partners. Causes include user input errors, inconsistent data schemas, transmission failures, and system crashes.

There are several types of missing data:

1. Missing completely at random (MCAR): No logical pattern to the absence
2. Missing at random (MAR): Missingness related to other observed variables
3. Missing not at random (MNAR): Missing values depend on unobserved data

Corrupt data can appear as unreadable file encodings, invalid data types (e.g., text in numeric columns), or logically inconsistent values (e.g., negative transaction amounts).

Consider this scenario:

A fintech company aggregates loan applications through a mobile platform. Some records are missing income data, while others show interest rates outside the valid range. These issues, if not addressed, will skew risk assessments and damage decision-making. The data team must clean and impute missing values before feeding them into machine learning models. Refer to the tools and platforms below:

1. Pandas (Python) – Standard for data wrangling
2. [OpenRefine](https://openrefine.org/) – GUI-based data cleaning tool
3. [Pyjanitor](https://pypi.org/project/pyjanitor/) – Enhancements for cleaner Pandas workflows
4. [DataCleaner](https://datacleaner.github.io/) – Lightweight data profiling and transformation tool

Techniques and Sample Codes:

1. Identifying Missing Data in Pandas

python

import pandas as pd

df = pd.read_csv('loans.csv')

print(df.isnull().sum()) # Count missing values per column

2. Handling Strategies
3. Drop rows or columns with excessive missing values
4. Impute using mean, median, mode, or domain logic
5. Flag missing entries as a new category (useful in ML)

python

df['income'] = df['income'].fillna(df['income'].median()) # Median imputation

df['loan_purpose'] = df['loan_purpose'].fillna('unknown') # Fill with placeholder

3. Detecting Corrupt or Inconsistent Values

python

# Example: find negative loan amounts

df[df['loan_amount'] < 0]

Strategies for scalable and resilient pipelines

Scalability refers to a system’s ability to handle increased load without degradation, while resilience is the ability to recover from failures or disruptions. In modern data ecosystems, where large volumes of data arrive continuously (e.g., transaction logs, sensor feeds, or user activity), pipelines must be designed to grow with demand and remain stable under pressure.

A scalable pipeline can add more processing power (scale out/in) or handle larger datasets without major reconfiguration. A resilient pipeline can restart failed tasks, retry connections, and continue processing despite partial failures.

Key features of resilient pipelines include:

1. Task-level retries
2. Logging and monitoring
3. Error isolation (fail one task, not the entire flow)
4. Decoupled components (microservices, queues)

Consider this scenario:

A financial reporting dashboard aggregates hourly data from ten services. During high-volume reporting periods, one service becomes unavailable, and the pipeline crashes. With resilience techniques (e.g., retry logic, dead-letter queues), the pipeline can isolate the failed source and proceed with partial data until recovery.

1. Apache Airflow – Modular DAG-based orchestration with retry and alerting
2. Apache NiFi – Visual data flow tool with built-in failover handling
3. Docker + Kubernetes – Container-based deployment for horizontal scaling
4. RabbitMQ or Kafka – For building decoupled, asynchronous pipelines

Techniques and Sample Code

1. Retry failed tasks in Airflow

python

from airflow import DAG

from airflow.operators.python_operator import PythonOperator

from datetime import datetime

def pull_data():

    # logic to fetch external API data

    pass

dag = DAG('resilient_pipeline', start_date=datetime(2023, 1, 1))

fetch = PythonOperator(

    task_id='fetch_api',

    python_callable=pull_data,

    retries=3,

    retry_delay=timedelta(minutes=5),

    dag=dag

)

2. Logging errors and continuing flow

python

try:

    result = fetch_data()

except Exception as e:

    log_error(e)

    result = default_fallback_data()

3. Queue-based decoupling (pseudo-example)
4. Producer sends data to a queue (e.g., Kafka topic)
5. Consumer picks data and processes independently This design ensures that if one consumer fails, others are unaffected.

Why don't you try working around this scenario:

You are managing the data infrastructure for a digital loan approval system. Your task is to build a pipeline that ingests application data, detects and handles missing or corrupt records, and ensures the pipeline continues running even if a source system goes offline temporarily.

Requirements

1. Use Python, Pandas, or Airflow to construct the pipeline
2. Implement logic to detect and clean incomplete records
3. Add retry or fallback logic for data ingestion tasks
4. Document any limitations and how your design handles growth or failure

Deliverables

1. Cleaned dataset (CSV or database)
2. Pipeline script with comments
3. Error handling/report logs

Short explanation of your scalability and resilience design.

The catch? This was the exact thing given to you, sent via a messaging app that converted the file into this type:

application_id,applicant_name,age,income,loan_amount,credit_score,application_date,status

A1001,Jane Cruz,29,35000,100000,720,2023-05-01,APPROVED

A1002,Eric Tan,45,40000,150000,,2023-05-02,REJECTED

A1003,Lisa Ong,,"25000",80000,690,2023-05-03,APPROVED

A1004,Juan Dela Cruz,35,38000,120000,680,2023-05-04,REVIEW

A1005,Mary Santos,27,NaN,110000,700,2023-05-05,APPROVED

A1006,,"33000",90000,710,2023-05-06,APPROVED

A1007,Alex Lim,31,37000,115000,695,,APPROVED

A1008,Rosa Reyes,40,42000,,"725",2023-05-07,REJECTED

A1009,John Chen,50,41000,200000,730,2023-05-08,REJECTED

A1010,Emily Yu,22,32000,90000,710,2023-05-09,APPROVED

A1011,Robert,38,0,120000,NaN,2023-05-10,

A1012,NaN,34,39000,110000,710,2023-05-11,REVIEW

A1013,Sophia,27,35000,95000,725,2023-05-12,APPROVED

A1014,Miguel Ramos,28,34000,100000,710,2023-05-13,APPROVED

A1015,Corrupt Entry,??,N/A,foo,bar,2023-05-14,UNKNOWN

---

# --- OUTPUT ---

---

# Milestone 2: Refined Project Prototype (Draft 1)

This task marks the beginning of your platform development journey. You are expected to set up the core components of your platform based on the initial plans and architecture outlined in Milestone 1. This will serve as the foundation for the more complex features you will build in later stages.

Each specialization will focus on a critical first output that reflects the platform’s initial functionality, connectivity, or data flow.

Follow the instructions below:

1. Review your Milestone 1 output—specifically your platform structure, feature outline, and implementation plan.
2. Set up the core technical environment:
3. Create and organize your project repository or file structure
4. Install required packages, frameworks, or dependencies
5. Create basic configuration files (e.g., .env,.yaml, or .json)
6. Develop the minimum functional version of one key feature or module.
7. Prepare short documentation explaining:
8. What you set up and why
9. Challenges you encountered
10. What worked and what needs refinement
11. Specialization-specific Tasks
12. For Software Development Track - You must deliver a functional prototype of your platform with one working core feature. This may include:
13. A basic user login module (with front-end and back-end logic)
14. A data sync function with local storage or API connection
15. Suggested tools: Node.js, Express, React/Vue, GitHub, Postman
16. For Network and Cybersecurity Track - You must deliver a securely configured network setup that simulates platform communication or protection. Your output may include:
17. A working VPN configuration
18. A firewall or routing rule that filters or prioritizes traffic
19. A performance validation report (e.g., usingiperf,nmap, or latency logs)
20. Suggested tools: pfSense, WireGuard, OpenVPN, Wireshark, IPTables
21. For Data Analytics Track
22. You must deliver a cleaned and partially processed dataset flowing through the start of your data pipeline. Your output may include:
23. Raw data cleaning and transformation scripts
24. Schema validation or null value handling
25. Initial storage of cleaned data in CSV, Parquet, or SQL
26. Suggested tools: Python (Pandas), Pandera, Airflow, Jupyter Notebook

---

# Reflection

Software Development

1. How did handling user input validation and basic authentication help you better understand the importance of error handling and security in production environments?
2. Reflecting on how your module integrates with others in the platform, what practices helped ensure that your code worked well in a team-based, multi-component system?

Network and Cybersecurity

1. In configuring firewalls or analyzing traffic during the simulated flash sale, what surprised you about how easily performance and security can conflict?
2. If you had to deploy your current setup in a fintech company handling financial data, what would you improve to ensure reliability and protection against attack?

Data Analytics

1. How did encountering missing or corrupt data change the way you think about real-world data quality and how businesses make decisions from imperfect information?
2. Reflecting on your approach to scalability and validation, how confident are you that your pipeline would work under real production load or unpredictable input formats?

---
